{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required packages and setup\n",
        "!pip install torch numpy tqdm\n",
        "\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import argparse\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from collections import Counter\n",
        "\n",
        "# Create data directory\n",
        "!mkdir -p data\n",
        "\n",
        "# Download MovieLens-1M dataset from the GitHub repository\n",
        "!wget -O data/ml-1m.txt https://raw.githubusercontent.com/pmixer/SASRec.pytorch/main/python/data/ml-1m.txt\n",
        "\n",
        "print(\"Dataset downloaded successfully!\")\n",
        "\n",
        "# Verify the dataset\n",
        "if os.path.exists('data/ml-1m.txt'):\n",
        "    with open('data/ml-1m.txt', 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    print(f\"Dataset loaded with {len(lines)} interactions\")\n",
        "    print(\"First few lines:\")\n",
        "    for i in range(min(5, len(lines))):\n",
        "        print(lines[i].strip())\n",
        "else:\n",
        "    print(\"Error: Dataset not found!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHS9zDwcNqzY",
        "outputId": "b14f6edf-6ab3-43d4-fada-2490f7508b01"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "--2025-06-07 02:00:29--  https://raw.githubusercontent.com/pmixer/SASRec.pytorch/main/python/data/ml-1m.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9053831 (8.6M) [text/plain]\n",
            "Saving to: ‘data/ml-1m.txt’\n",
            "\n",
            "data/ml-1m.txt      100%[===================>]   8.63M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-06-07 02:00:30 (61.3 MB/s) - ‘data/ml-1m.txt’ saved [9053831/9053831]\n",
            "\n",
            "Dataset downloaded successfully!\n",
            "Dataset loaded with 999611 interactions\n",
            "First few lines:\n",
            "1 1\n",
            "1 2\n",
            "1 3\n",
            "1 4\n",
            "1 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def prepare_and_download_dataset(dataset_name):\n",
        "    \"\"\"\n",
        "    Downloads and prepares amazon-book or yelp2018 datasets for SASRec.\n",
        "    \"\"\"\n",
        "    final_path = f'data/{dataset_name}.txt'\n",
        "    if os.path.exists(final_path):\n",
        "        print(f\"Dataset '{dataset_name}' already prepared.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Preparing dataset: {dataset_name}...\")\n",
        "    base_url = f\"https://raw.githubusercontent.com/gusye1234/LightGCN-PyTorch/master/data/{dataset_name}\"\n",
        "\n",
        "    all_interactions = []\n",
        "\n",
        "    # Download and process train.txt and test.txt\n",
        "    for file_part in ['train', 'test']:\n",
        "        url = f\"{base_url}/{file_part}.txt\"\n",
        "        try:\n",
        "            res = requests.get(url)\n",
        "            res.raise_for_status()\n",
        "\n",
        "            lines = res.text.strip().split('\\n')\n",
        "            for line in lines:\n",
        "                parts = line.strip().split()\n",
        "                if not parts: continue\n",
        "\n",
        "                # Convert to 1-based indexing for SASRec compatibility\n",
        "                user_id = int(parts[0]) + 1\n",
        "                item_ids = [int(i) + 1 for i in parts[1:]]\n",
        "\n",
        "                for item_id in item_ids:\n",
        "                    all_interactions.append((user_id, item_id))\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error downloading {url}: {e}\")\n",
        "            return\n",
        "\n",
        "    # Write to the final format required by SASRec\n",
        "    with open(final_path, 'w') as f:\n",
        "        for u, i in all_interactions:\n",
        "            f.write(f'{u} {i}\\n')\n",
        "\n",
        "    print(f\"Successfully prepared and saved dataset '{dataset_name}' to {final_path}\")"
      ],
      "metadata": {
        "id": "e0fvuRpgGN8i"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aSTY7xqDGFQ_"
      },
      "outputs": [],
      "source": [
        "# utils.py\n",
        "import sys\n",
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from multiprocessing import Process, Queue\n",
        "\n",
        "def build_index(dataset_name):\n",
        "    ui_mat = np.loadtxt('data/%s.txt' % dataset_name, dtype=np.int32)\n",
        "\n",
        "    n_users = ui_mat[:, 0].max()\n",
        "    n_items = ui_mat[:, 1].max()\n",
        "\n",
        "    u2i_index = [[] for _ in range(n_users + 1)]\n",
        "    i2u_index = [[] for _ in range(n_items + 1)]\n",
        "\n",
        "    for ui_pair in ui_mat:\n",
        "        u2i_index[ui_pair[0]].append(ui_pair[1])\n",
        "        i2u_index[ui_pair[1]].append(ui_pair[0])\n",
        "\n",
        "    return u2i_index, i2u_index\n",
        "\n",
        "# sampler for batch generation\n",
        "def random_neq(l, r, s):\n",
        "    t = np.random.randint(l, r)\n",
        "    while t in s:\n",
        "        t = np.random.randint(l, r)\n",
        "    return t\n",
        "\n",
        "def sample_function(user_train, usernum, itemnum, batch_size, maxlen, result_queue, SEED):\n",
        "    def sample(uid):\n",
        "        # uid = np.random.randint(1, usernum + 1)\n",
        "        while len(user_train[uid]) <= 1: uid = np.random.randint(1, usernum + 1)\n",
        "\n",
        "        seq = np.zeros([maxlen], dtype=np.int32)\n",
        "        pos = np.zeros([maxlen], dtype=np.int32)\n",
        "        neg = np.zeros([maxlen], dtype=np.int32)\n",
        "        nxt = user_train[uid][-1]\n",
        "        idx = maxlen - 1\n",
        "\n",
        "        ts = set(user_train[uid])\n",
        "        for i in reversed(user_train[uid][:-1]):\n",
        "            seq[idx] = i\n",
        "            pos[idx] = nxt\n",
        "            if nxt != 0: neg[idx] = random_neq(1, itemnum + 1, ts)\n",
        "            nxt = i\n",
        "            idx -= 1\n",
        "            if idx == -1: break\n",
        "\n",
        "        return (uid, seq, pos, neg)\n",
        "\n",
        "    np.random.seed(SEED)\n",
        "    uids = np.arange(1, usernum+1, dtype=np.int32)\n",
        "    counter = 0\n",
        "    while True:\n",
        "        if counter % usernum == 0:\n",
        "            np.random.shuffle(uids)\n",
        "        one_batch = []\n",
        "        for i in range(batch_size):\n",
        "            one_batch.append(sample(uids[counter % usernum]))\n",
        "            counter += 1\n",
        "        result_queue.put(zip(*one_batch))\n",
        "\n",
        "class WarpSampler(object):\n",
        "    def __init__(self, User, usernum, itemnum, batch_size=64, maxlen=10, n_workers=1):\n",
        "        self.result_queue = Queue(maxsize=n_workers * 10)\n",
        "        self.processors = []\n",
        "        for i in range(n_workers):\n",
        "            self.processors.append(\n",
        "                Process(target=sample_function, args=(User,\n",
        "                                                      usernum,\n",
        "                                                      itemnum,\n",
        "                                                      batch_size,\n",
        "                                                      maxlen,\n",
        "                                                      self.result_queue,\n",
        "                                                      np.random.randint(2e9)\n",
        "                                                      )))\n",
        "            self.processors[-1].daemon = True\n",
        "            self.processors[-1].start()\n",
        "\n",
        "    def next_batch(self):\n",
        "        return self.result_queue.get()\n",
        "\n",
        "    def close(self):\n",
        "        for p in self.processors:\n",
        "            p.terminate()\n",
        "            p.join()\n",
        "\n",
        "# train/val/test data generation\n",
        "def data_partition(fname):\n",
        "    usernum = 0\n",
        "    itemnum = 0\n",
        "    User = defaultdict(list)\n",
        "    user_train = {}\n",
        "    user_valid = {}\n",
        "    user_test = {}\n",
        "    # assume user/item index starting from 1\n",
        "    f = open('data/%s.txt' % fname, 'r')\n",
        "    for line in f:\n",
        "        u, i = line.rstrip().split(' ')\n",
        "        u = int(u)\n",
        "        i = int(i)\n",
        "        usernum = max(u, usernum)\n",
        "        itemnum = max(i, itemnum)\n",
        "        User[u].append(i)\n",
        "\n",
        "    for user in User:\n",
        "        nfeedback = len(User[user])\n",
        "        if nfeedback < 3:\n",
        "            user_train[user] = User[user]\n",
        "            user_valid[user] = []\n",
        "            user_test[user] = []\n",
        "        else:\n",
        "            user_train[user] = User[user][:-2]\n",
        "            user_valid[user] = []\n",
        "            user_valid[user].append(User[user][-2])\n",
        "            user_test[user] = []\n",
        "            user_test[user].append(User[user][-1])\n",
        "    return [user_train, user_valid, user_test, usernum, itemnum]\n",
        "\n",
        "# TODO: merge evaluate functions for test and val set\n",
        "# evaluate on test set\n",
        "def evaluate(model, dataset, args):\n",
        "    [train, valid, test, usernum, itemnum] = copy.deepcopy(dataset)\n",
        "\n",
        "    NDCG = 0.0\n",
        "    HT = 0.0\n",
        "    valid_user = 0.0\n",
        "\n",
        "    if usernum>10000:\n",
        "        users = random.sample(range(1, usernum + 1), 10000)\n",
        "    else:\n",
        "        users = range(1, usernum + 1)\n",
        "    for u in users:\n",
        "\n",
        "        if len(train[u]) < 1 or len(test[u]) < 1: continue\n",
        "\n",
        "        seq = np.zeros([args.maxlen], dtype=np.int32)\n",
        "        idx = args.maxlen - 1\n",
        "        seq[idx] = valid[u][0]\n",
        "        idx -= 1\n",
        "        for i in reversed(train[u]):\n",
        "            seq[idx] = i\n",
        "            idx -= 1\n",
        "            if idx == -1: break\n",
        "        rated = set(train[u])\n",
        "        rated.add(0)\n",
        "        item_idx = [test[u][0]]\n",
        "        for _ in range(100):\n",
        "            t = np.random.randint(1, itemnum + 1)\n",
        "            while t in rated: t = np.random.randint(1, itemnum + 1)\n",
        "            item_idx.append(t)\n",
        "\n",
        "        predictions = -model.predict(*[np.array(l) for l in [[u], [seq], item_idx]])\n",
        "        predictions = predictions[0] # - for 1st argsort DESC\n",
        "\n",
        "        rank = predictions.argsort().argsort()[0].item()\n",
        "\n",
        "        valid_user += 1\n",
        "\n",
        "        if rank < 10:\n",
        "            NDCG += 1 / np.log2(rank + 2)\n",
        "            HT += 1\n",
        "        if valid_user % 100 == 0:\n",
        "            print('.', end=\"\")\n",
        "            sys.stdout.flush()\n",
        "\n",
        "    return NDCG / valid_user, HT / valid_user\n",
        "\n",
        "# evaluate on val set\n",
        "def evaluate_valid(model, dataset, args):\n",
        "    [train, valid, test, usernum, itemnum] = copy.deepcopy(dataset)\n",
        "\n",
        "    NDCG = 0.0\n",
        "    valid_user = 0.0\n",
        "    HT = 0.0\n",
        "    if usernum>10000:\n",
        "        users = random.sample(range(1, usernum + 1), 10000)\n",
        "    else:\n",
        "        users = range(1, usernum + 1)\n",
        "    for u in users:\n",
        "        if len(train[u]) < 1 or len(valid[u]) < 1: continue\n",
        "\n",
        "        seq = np.zeros([args.maxlen], dtype=np.int32)\n",
        "        idx = args.maxlen - 1\n",
        "        for i in reversed(train[u]):\n",
        "            seq[idx] = i\n",
        "            idx -= 1\n",
        "            if idx == -1: break\n",
        "\n",
        "        rated = set(train[u])\n",
        "        rated.add(0)\n",
        "        item_idx = [valid[u][0]]\n",
        "        for _ in range(100):\n",
        "            t = np.random.randint(1, itemnum + 1)\n",
        "            while t in rated: t = np.random.randint(1, itemnum + 1)\n",
        "            item_idx.append(t)\n",
        "\n",
        "        predictions = -model.predict(*[np.array(l) for l in [[u], [seq], item_idx]])\n",
        "        predictions = predictions[0]\n",
        "\n",
        "        rank = predictions.argsort().argsort()[0].item()\n",
        "\n",
        "        valid_user += 1\n",
        "\n",
        "        if rank < 10:\n",
        "            NDCG += 1 / np.log2(rank + 2)\n",
        "            HT += 1\n",
        "        if valid_user % 100 == 0:\n",
        "            print('.', end=\"\")\n",
        "            sys.stdout.flush()\n",
        "\n",
        "    return NDCG / valid_user, HT / valid_user"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.py\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class PointWiseFeedForward(torch.nn.Module):\n",
        "    def __init__(self, hidden_units, dropout_rate):\n",
        "\n",
        "        super(PointWiseFeedForward, self).__init__()\n",
        "\n",
        "        self.conv1 = torch.nn.Conv1d(hidden_units, hidden_units, kernel_size=1)\n",
        "        self.dropout1 = torch.nn.Dropout(p=dropout_rate)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.conv2 = torch.nn.Conv1d(hidden_units, hidden_units, kernel_size=1)\n",
        "        self.dropout2 = torch.nn.Dropout(p=dropout_rate)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.dropout2(self.conv2(self.relu(self.dropout1(self.conv1(inputs.transpose(-1, -2))))))\n",
        "        outputs = outputs.transpose(-1, -2) # as Conv1D requires (N, C, Length)\n",
        "        return outputs\n",
        "\n",
        "# pls use the following self-made multihead attention layer\n",
        "# in case your pytorch version is below 1.16 or for other reasons\n",
        "# https://github.com/pmixer/TiSASRec.pytorch/blob/master/model.py\n",
        "\n",
        "class SASRec(torch.nn.Module):\n",
        "    def __init__(self, user_num, item_num, args):\n",
        "        super(SASRec, self).__init__()\n",
        "\n",
        "        self.user_num = user_num\n",
        "        self.item_num = item_num\n",
        "        self.dev = args.device\n",
        "        self.norm_first = args.norm_first\n",
        "\n",
        "        # TODO: loss += args.l2_emb for regularizing embedding vectors during training\n",
        "        # https://stackoverflow.com/questions/42704283/adding-l1-l2-regularization-in-pytorch\n",
        "        self.item_emb = torch.nn.Embedding(self.item_num+1, args.hidden_units, padding_idx=0)\n",
        "        self.pos_emb = torch.nn.Embedding(args.maxlen+1, args.hidden_units, padding_idx=0)\n",
        "        self.emb_dropout = torch.nn.Dropout(p=args.dropout_rate)\n",
        "\n",
        "        self.attention_layernorms = torch.nn.ModuleList() # to be Q for self-attention\n",
        "        self.attention_layers = torch.nn.ModuleList()\n",
        "        self.forward_layernorms = torch.nn.ModuleList()\n",
        "        self.forward_layers = torch.nn.ModuleList()\n",
        "\n",
        "        self.last_layernorm = torch.nn.LayerNorm(args.hidden_units, eps=1e-8)\n",
        "\n",
        "        for _ in range(args.num_blocks):\n",
        "            new_attn_layernorm = torch.nn.LayerNorm(args.hidden_units, eps=1e-8)\n",
        "            self.attention_layernorms.append(new_attn_layernorm)\n",
        "\n",
        "            new_attn_layer =  torch.nn.MultiheadAttention(args.hidden_units,\n",
        "                                                            args.num_heads,\n",
        "                                                            args.dropout_rate)\n",
        "            self.attention_layers.append(new_attn_layer)\n",
        "\n",
        "            new_fwd_layernorm = torch.nn.LayerNorm(args.hidden_units, eps=1e-8)\n",
        "            self.forward_layernorms.append(new_fwd_layernorm)\n",
        "\n",
        "            new_fwd_layer = PointWiseFeedForward(args.hidden_units, args.dropout_rate)\n",
        "            self.forward_layers.append(new_fwd_layer)\n",
        "\n",
        "            # self.pos_sigmoid = torch.nn.Sigmoid()\n",
        "            # self.neg_sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    def log2feats(self, log_seqs): # TODO: fp64 and int64 as default in python, trim?\n",
        "        seqs = self.item_emb(torch.LongTensor(log_seqs).to(self.dev))\n",
        "        seqs *= self.item_emb.embedding_dim ** 0.5\n",
        "        poss = np.tile(np.arange(1, log_seqs.shape[1] + 1), [log_seqs.shape[0], 1])\n",
        "        # TODO: directly do tensor = torch.arange(1, xxx, device='cuda') to save extra overheads\n",
        "        poss *= (log_seqs != 0)\n",
        "        seqs += self.pos_emb(torch.LongTensor(poss).to(self.dev))\n",
        "        seqs = self.emb_dropout(seqs)\n",
        "\n",
        "        tl = seqs.shape[1] # time dim len for enforce causality\n",
        "        attention_mask = ~torch.tril(torch.ones((tl, tl), dtype=torch.bool, device=self.dev))\n",
        "\n",
        "        for i in range(len(self.attention_layers)):\n",
        "            seqs = torch.transpose(seqs, 0, 1)\n",
        "            if self.norm_first:\n",
        "                x = self.attention_layernorms[i](seqs)\n",
        "                mha_outputs, _ = self.attention_layers[i](x, x, x,\n",
        "                                                attn_mask=attention_mask)\n",
        "                seqs = seqs + mha_outputs\n",
        "                seqs = torch.transpose(seqs, 0, 1)\n",
        "                seqs = seqs + self.forward_layers[i](self.forward_layernorms[i](seqs))\n",
        "            else:\n",
        "                mha_outputs, _ = self.attention_layers[i](seqs, seqs, seqs,\n",
        "                                                attn_mask=attention_mask)\n",
        "                seqs = self.attention_layernorms[i](seqs + mha_outputs)\n",
        "                seqs = torch.transpose(seqs, 0, 1)\n",
        "                seqs = self.forward_layernorms[i](seqs + self.forward_layers[i](seqs))\n",
        "\n",
        "        log_feats = self.last_layernorm(seqs) # (U, T, C) -> (U, -1, C)\n",
        "\n",
        "        return log_feats\n",
        "\n",
        "    def forward(self, user_ids, log_seqs, pos_seqs, neg_seqs): # for training\n",
        "        log_feats = self.log2feats(log_seqs) # user_ids hasn't been used yet\n",
        "\n",
        "        pos_embs = self.item_emb(torch.LongTensor(pos_seqs).to(self.dev))\n",
        "        neg_embs = self.item_emb(torch.LongTensor(neg_seqs).to(self.dev))\n",
        "\n",
        "        pos_logits = (log_feats * pos_embs).sum(dim=-1)\n",
        "        neg_logits = (log_feats * neg_embs).sum(dim=-1)\n",
        "\n",
        "        # pos_pred = self.pos_sigmoid(pos_logits)\n",
        "        # neg_pred = self.neg_sigmoid(neg_logits)\n",
        "\n",
        "        return pos_logits, neg_logits # pos_pred, neg_pred\n",
        "\n",
        "    def predict(self, user_ids, log_seqs, item_indices): # for inference\n",
        "        log_feats = self.log2feats(log_seqs) # user_ids hasn't been used yet\n",
        "\n",
        "        final_feat = log_feats[:, -1, :] # only use last QKV classifier, a waste\n",
        "\n",
        "        item_embs = self.item_emb(torch.LongTensor(item_indices).to(self.dev)) # (U, I, C)\n",
        "\n",
        "        logits = item_embs.matmul(final_feat.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        # preds = self.pos_sigmoid(logits) # rank same item list for different users\n",
        "\n",
        "        return logits # preds # (U, I)"
      ],
      "metadata": {
        "id": "1YwS_tqRO0PJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main.py\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import argparse\n",
        "\n",
        "# Import our modules\n",
        "# from model import SASRec\n",
        "# from utils import *\n",
        "\n",
        "def str2bool(s):\n",
        "    if s not in {'false', 'true'}:\n",
        "        raise ValueError('Not a valid boolean string')\n",
        "    return s == 'true'\n",
        "\n",
        "# For Colab, we'll set arguments directly instead of using argparse\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.dataset = 'ml-1m'\n",
        "        self.train_dir = 'default'\n",
        "        self.batch_size = 128\n",
        "        self.lr = 0.001\n",
        "        self.maxlen = 200\n",
        "        self.hidden_units = 50\n",
        "        self.num_blocks = 2\n",
        "        self.num_epochs = 201  # Reduced for demo\n",
        "        self.num_heads = 1\n",
        "        self.dropout_rate = 0.2\n",
        "        self.l2_emb = 0.0\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.inference_only = False\n",
        "        self.state_dict_path = None\n",
        "        self.norm_first = False\n",
        "\n",
        "args = Args()\n",
        "\n",
        "# Create output directory\n",
        "if not os.path.isdir(args.dataset + '_' + args.train_dir):\n",
        "    os.makedirs(args.dataset + '_' + args.train_dir)\n",
        "\n",
        "with open(os.path.join(args.dataset + '_' + args.train_dir, 'args.txt'), 'w') as f:\n",
        "    f.write('\\n'.join([str(k) + ',' + str(v) for k, v in sorted(vars(args).items(), key=lambda x: x[0])]))\n",
        "f.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Define all datasets to run\n",
        "    datasets_to_run = ['ml-1m', 'amazon-book', 'yelp2018']\n",
        "\n",
        "    for dataset_name in datasets_to_run:\n",
        "        print(f\"\\n\\n{'='*60}\")\n",
        "        print(f\"RUNNING SASREC ON: {dataset_name.upper()}\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "        # Set arguments for the current run\n",
        "        args = Args()\n",
        "        args.dataset = dataset_name\n",
        "\n",
        "        # Adjust maxlen for larger datasets if necessary\n",
        "        if dataset_name in ['amazon-book', 'yelp2018']:\n",
        "            args.maxlen = 50 # These datasets have shorter average sequences\n",
        "        else:\n",
        "            args.maxlen = 200 # Original value for ml-1m\n",
        "\n",
        "        # Prepare the dataset if it's not ml-1m\n",
        "        if dataset_name != 'ml-1m':\n",
        "            prepare_and_download_dataset(dataset_name)\n",
        "\n",
        "        # Create output directory for the current dataset\n",
        "        log_dir = args.dataset + '_' + args.train_dir\n",
        "        if not os.path.isdir(log_dir):\n",
        "            os.makedirs(log_dir)\n",
        "        with open(os.path.join(log_dir, 'args.txt'), 'w') as f:\n",
        "            f.write('\\n'.join([str(k) + ',' + str(v) for k, v in sorted(vars(args).items(), key=lambda x: x[0])]))\n",
        "        f.close()\n",
        "\n",
        "        # Load and partition the dataset\n",
        "        dataset = data_partition(args.dataset)\n",
        "        [user_train, user_valid, user_test, usernum, itemnum] = dataset\n",
        "        num_batch = (len(user_train) - 1) // args.batch_size + 1\n",
        "\n",
        "        cc = sum(len(user_train[u]) for u in user_train)\n",
        "        print('average sequence length: %.2f' % (cc / len(user_train)))\n",
        "\n",
        "        # Setup logging\n",
        "        log_file = open(os.path.join(log_dir, 'log.txt'), 'w')\n",
        "        log_file.write('epoch (val_ndcg, val_hr) (test_ndcg, test_hr)\\n')\n",
        "\n",
        "        # Initialize sampler and model\n",
        "        sampler = WarpSampler(user_train, usernum, itemnum, batch_size=args.batch_size, maxlen=args.maxlen, n_workers=3)\n",
        "        model = SASRec(usernum, itemnum, args).to(args.device)\n",
        "\n",
        "        # Initialize weights\n",
        "        for name, param in model.named_parameters():\n",
        "            try:\n",
        "                torch.nn.init.xavier_normal_(param.data)\n",
        "            except:\n",
        "                pass # ignore failed init layers\n",
        "        model.train()\n",
        "\n",
        "        bce_criterion = torch.nn.BCEWithLogitsLoss()\n",
        "        adam_optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, betas=(0.9, 0.98))\n",
        "\n",
        "        T = 0.0\n",
        "        t0 = time.time()\n",
        "\n",
        "        # Start training loop for the current dataset\n",
        "        for epoch in range(1, args.num_epochs + 1):\n",
        "            total_loss = 0.0\n",
        "            for step in range(num_batch):\n",
        "                u, seq, pos, neg = sampler.next_batch()\n",
        "                u, seq, pos, neg = np.array(u), np.array(seq), np.array(pos), np.array(neg)\n",
        "\n",
        "                pos_logits, neg_logits = model(u, seq, pos, neg)\n",
        "                pos_labels, neg_labels = torch.ones(pos_logits.shape, device=args.device), torch.zeros(neg_logits.shape, device=args.device)\n",
        "\n",
        "                adam_optimizer.zero_grad()\n",
        "                indices = np.where(pos != 0)\n",
        "                loss = bce_criterion(pos_logits[indices], pos_labels[indices])\n",
        "                loss += bce_criterion(neg_logits[indices], neg_labels[indices])\n",
        "                for param in model.item_emb.parameters(): loss += args.l2_emb * torch.norm(param)\n",
        "                loss.backward()\n",
        "                adam_optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            print(f\"Epoch: {epoch}, Average Loss: {total_loss / num_batch:.4f}\")\n",
        "\n",
        "            if epoch % 20 == 0:\n",
        "                model.eval()\n",
        "                t1 = time.time() - t0\n",
        "                T += t1\n",
        "                print('Evaluating', end='')\n",
        "\n",
        "                # NOTE: Using a single merged evaluation function is recommended\n",
        "                # For now, we use the original ones\n",
        "                t_test = evaluate(model, dataset, args)\n",
        "                t_valid = evaluate_valid(model, dataset, args)\n",
        "\n",
        "                print('epoch:%d, time: %f(s), valid (NDCG@10: %.4f, HR@10: %.4f), test (NDCG@10: %.4f, HR@10: %.4f)'\n",
        "                        % (epoch, T, t_valid[0], t_valid[1], t_test[0], t_test[1]))\n",
        "\n",
        "                log_file.write(str(epoch) + ' ' + str(t_valid) + ' ' + str(t_test) + '\\n')\n",
        "                log_file.flush()\n",
        "                t0 = time.time()\n",
        "                model.train()\n",
        "\n",
        "        log_file.close()\n",
        "        sampler.close()\n",
        "        print(f\"Done training on {dataset_name}\")\n",
        "\n",
        "    print(\"\\n\\nAll dataset runs completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0shBAPoYPAU4",
        "outputId": "9e1aab80-bed8-4d59-d7b0-349fc8fcc489"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "============================================================\n",
            "RUNNING SASREC ON: ML-1M\n",
            "============================================================\n",
            "\n",
            "average sequence length: 163.50\n",
            "Epoch: 1, Average Loss: 1.1686\n",
            "Epoch: 2, Average Loss: 1.0023\n",
            "Epoch: 3, Average Loss: 0.9566\n",
            "Epoch: 4, Average Loss: 0.8812\n",
            "Epoch: 5, Average Loss: 0.8251\n",
            "Epoch: 6, Average Loss: 0.7888\n",
            "Epoch: 7, Average Loss: 0.7465\n",
            "Epoch: 8, Average Loss: 0.7072\n",
            "Epoch: 9, Average Loss: 0.6878\n",
            "Epoch: 10, Average Loss: 0.6652\n",
            "Epoch: 11, Average Loss: 0.6445\n",
            "Epoch: 12, Average Loss: 0.6371\n",
            "Epoch: 13, Average Loss: 0.6212\n",
            "Epoch: 14, Average Loss: 0.6054\n",
            "Epoch: 15, Average Loss: 0.5977\n",
            "Epoch: 16, Average Loss: 0.5873\n",
            "Epoch: 17, Average Loss: 0.5746\n",
            "Epoch: 18, Average Loss: 0.5696\n",
            "Epoch: 19, Average Loss: 0.5605\n",
            "Epoch: 20, Average Loss: 0.5508\n",
            "Evaluating........................................................................................................................epoch:20, time: 61.861300(s), valid (NDCG@10: 0.5382, HR@10: 0.7967), test (NDCG@10: 0.5197, HR@10: 0.7687)\n",
            "Epoch: 21, Average Loss: 0.5489\n",
            "Epoch: 22, Average Loss: 0.5369\n",
            "Epoch: 23, Average Loss: 0.5308\n",
            "Epoch: 24, Average Loss: 0.5302\n",
            "Epoch: 25, Average Loss: 0.5248\n",
            "Epoch: 26, Average Loss: 0.5181\n",
            "Epoch: 27, Average Loss: 0.5164\n",
            "Epoch: 28, Average Loss: 0.5131\n",
            "Epoch: 29, Average Loss: 0.5086\n",
            "Epoch: 30, Average Loss: 0.5070\n",
            "Epoch: 31, Average Loss: 0.5025\n",
            "Epoch: 32, Average Loss: 0.4983\n",
            "Epoch: 33, Average Loss: 0.4992\n",
            "Epoch: 34, Average Loss: 0.4918\n",
            "Epoch: 35, Average Loss: 0.4898\n",
            "Epoch: 36, Average Loss: 0.4916\n",
            "Epoch: 37, Average Loss: 0.4839\n",
            "Epoch: 38, Average Loss: 0.4848\n",
            "Epoch: 39, Average Loss: 0.4864\n",
            "Epoch: 40, Average Loss: 0.4770\n",
            "Evaluating........................................................................................................................epoch:40, time: 115.541667(s), valid (NDCG@10: 0.5839, HR@10: 0.8281), test (NDCG@10: 0.5591, HR@10: 0.8018)\n",
            "Epoch: 41, Average Loss: 0.4793\n",
            "Epoch: 42, Average Loss: 0.4797\n",
            "Epoch: 43, Average Loss: 0.4722\n",
            "Epoch: 44, Average Loss: 0.4746\n",
            "Epoch: 45, Average Loss: 0.4752\n",
            "Epoch: 46, Average Loss: 0.4675\n",
            "Epoch: 47, Average Loss: 0.4706\n",
            "Epoch: 48, Average Loss: 0.4646\n",
            "Epoch: 49, Average Loss: 0.4641\n",
            "Epoch: 50, Average Loss: 0.4697\n",
            "Epoch: 51, Average Loss: 0.4620\n",
            "Epoch: 52, Average Loss: 0.4603\n",
            "Epoch: 53, Average Loss: 0.4621\n",
            "Epoch: 54, Average Loss: 0.4588\n",
            "Epoch: 55, Average Loss: 0.4576\n",
            "Epoch: 56, Average Loss: 0.4577\n",
            "Epoch: 57, Average Loss: 0.4567\n",
            "Epoch: 58, Average Loss: 0.4555\n",
            "Epoch: 59, Average Loss: 0.4529\n",
            "Epoch: 60, Average Loss: 0.4539\n",
            "Evaluating........................................................................................................................epoch:60, time: 168.807717(s), valid (NDCG@10: 0.5983, HR@10: 0.8358), test (NDCG@10: 0.5754, HR@10: 0.8076)\n",
            "Epoch: 61, Average Loss: 0.4523\n",
            "Epoch: 62, Average Loss: 0.4499\n",
            "Epoch: 63, Average Loss: 0.4488\n",
            "Epoch: 64, Average Loss: 0.4501\n",
            "Epoch: 65, Average Loss: 0.4495\n",
            "Epoch: 66, Average Loss: 0.4455\n",
            "Epoch: 67, Average Loss: 0.4474\n",
            "Epoch: 68, Average Loss: 0.4465\n",
            "Epoch: 69, Average Loss: 0.4434\n",
            "Epoch: 70, Average Loss: 0.4459\n",
            "Epoch: 71, Average Loss: 0.4403\n",
            "Epoch: 72, Average Loss: 0.4429\n",
            "Epoch: 73, Average Loss: 0.4429\n",
            "Epoch: 74, Average Loss: 0.4386\n",
            "Epoch: 75, Average Loss: 0.4393\n",
            "Epoch: 76, Average Loss: 0.4401\n",
            "Epoch: 77, Average Loss: 0.4360\n",
            "Epoch: 78, Average Loss: 0.4356\n",
            "Epoch: 79, Average Loss: 0.4406\n",
            "Epoch: 80, Average Loss: 0.4333\n",
            "Evaluating........................................................................................................................epoch:80, time: 221.956305(s), valid (NDCG@10: 0.6088, HR@10: 0.8432), test (NDCG@10: 0.5759, HR@10: 0.8134)\n",
            "Epoch: 81, Average Loss: 0.4343\n",
            "Epoch: 82, Average Loss: 0.4389\n",
            "Epoch: 83, Average Loss: 0.4345\n",
            "Epoch: 84, Average Loss: 0.4328\n",
            "Epoch: 85, Average Loss: 0.4350\n",
            "Epoch: 86, Average Loss: 0.4304\n",
            "Epoch: 87, Average Loss: 0.4314\n",
            "Epoch: 88, Average Loss: 0.4340\n",
            "Epoch: 89, Average Loss: 0.4302\n",
            "Epoch: 90, Average Loss: 0.4297\n",
            "Epoch: 91, Average Loss: 0.4318\n",
            "Epoch: 92, Average Loss: 0.4286\n",
            "Epoch: 93, Average Loss: 0.4314\n",
            "Epoch: 94, Average Loss: 0.4274\n",
            "Epoch: 95, Average Loss: 0.4287\n",
            "Epoch: 96, Average Loss: 0.4245\n",
            "Epoch: 97, Average Loss: 0.4274\n",
            "Epoch: 98, Average Loss: 0.4295\n",
            "Epoch: 99, Average Loss: 0.4247\n",
            "Epoch: 100, Average Loss: 0.4267\n",
            "Evaluating........................................................................................................................epoch:100, time: 276.119364(s), valid (NDCG@10: 0.6126, HR@10: 0.8462), test (NDCG@10: 0.5842, HR@10: 0.8141)\n",
            "Epoch: 101, Average Loss: 0.4266\n",
            "Epoch: 102, Average Loss: 0.4237\n",
            "Epoch: 103, Average Loss: 0.4221\n",
            "Epoch: 104, Average Loss: 0.4246\n",
            "Epoch: 105, Average Loss: 0.4230\n",
            "Epoch: 106, Average Loss: 0.4226\n",
            "Epoch: 107, Average Loss: 0.4245\n",
            "Epoch: 108, Average Loss: 0.4222\n",
            "Epoch: 109, Average Loss: 0.4220\n",
            "Epoch: 110, Average Loss: 0.4221\n",
            "Epoch: 111, Average Loss: 0.4206\n",
            "Epoch: 112, Average Loss: 0.4203\n",
            "Epoch: 113, Average Loss: 0.4212\n",
            "Epoch: 114, Average Loss: 0.4198\n",
            "Epoch: 115, Average Loss: 0.4207\n",
            "Epoch: 116, Average Loss: 0.4187\n",
            "Epoch: 117, Average Loss: 0.4192\n",
            "Epoch: 118, Average Loss: 0.4192\n",
            "Epoch: 119, Average Loss: 0.4199\n",
            "Epoch: 120, Average Loss: 0.4162\n",
            "Evaluating........................................................................................................................epoch:120, time: 331.250049(s), valid (NDCG@10: 0.6172, HR@10: 0.8478), test (NDCG@10: 0.5815, HR@10: 0.8162)\n",
            "Epoch: 121, Average Loss: 0.4194\n",
            "Epoch: 122, Average Loss: 0.4198\n",
            "Epoch: 123, Average Loss: 0.4158\n",
            "Epoch: 124, Average Loss: 0.4185\n",
            "Epoch: 125, Average Loss: 0.4165\n",
            "Epoch: 126, Average Loss: 0.4168\n",
            "Epoch: 127, Average Loss: 0.4162\n",
            "Epoch: 128, Average Loss: 0.4174\n",
            "Epoch: 129, Average Loss: 0.4151\n",
            "Epoch: 130, Average Loss: 0.4151\n",
            "Epoch: 131, Average Loss: 0.4181\n",
            "Epoch: 132, Average Loss: 0.4141\n",
            "Epoch: 133, Average Loss: 0.4131\n",
            "Epoch: 134, Average Loss: 0.4190\n",
            "Epoch: 135, Average Loss: 0.4136\n",
            "Epoch: 136, Average Loss: 0.4126\n",
            "Epoch: 137, Average Loss: 0.4169\n",
            "Epoch: 138, Average Loss: 0.4123\n",
            "Epoch: 139, Average Loss: 0.4136\n",
            "Epoch: 140, Average Loss: 0.4135\n",
            "Evaluating........................................................................................................................epoch:140, time: 389.100976(s), valid (NDCG@10: 0.6166, HR@10: 0.8472), test (NDCG@10: 0.5847, HR@10: 0.8177)\n",
            "Epoch: 141, Average Loss: 0.4099\n",
            "Epoch: 142, Average Loss: 0.4138\n",
            "Epoch: 143, Average Loss: 0.4091\n",
            "Epoch: 144, Average Loss: 0.4145\n",
            "Epoch: 145, Average Loss: 0.4131\n",
            "Epoch: 146, Average Loss: 0.4118\n",
            "Epoch: 147, Average Loss: 0.4106\n",
            "Epoch: 148, Average Loss: 0.4127\n",
            "Epoch: 149, Average Loss: 0.4127\n",
            "Epoch: 150, Average Loss: 0.4111\n",
            "Epoch: 151, Average Loss: 0.4120\n",
            "Epoch: 152, Average Loss: 0.4117\n",
            "Epoch: 153, Average Loss: 0.4102\n",
            "Epoch: 154, Average Loss: 0.4117\n",
            "Epoch: 155, Average Loss: 0.4092\n",
            "Epoch: 156, Average Loss: 0.4096\n",
            "Epoch: 157, Average Loss: 0.4107\n",
            "Epoch: 158, Average Loss: 0.4093\n",
            "Epoch: 159, Average Loss: 0.4079\n",
            "Epoch: 160, Average Loss: 0.4131\n",
            "Evaluating........................................................................................................................epoch:160, time: 443.721968(s), valid (NDCG@10: 0.6220, HR@10: 0.8492), test (NDCG@10: 0.5907, HR@10: 0.8180)\n",
            "Epoch: 161, Average Loss: 0.4082\n",
            "Epoch: 162, Average Loss: 0.4062\n",
            "Epoch: 163, Average Loss: 0.4113\n",
            "Epoch: 164, Average Loss: 0.4100\n",
            "Epoch: 165, Average Loss: 0.4056\n",
            "Epoch: 166, Average Loss: 0.4103\n",
            "Epoch: 167, Average Loss: 0.4095\n",
            "Epoch: 168, Average Loss: 0.4058\n",
            "Epoch: 169, Average Loss: 0.4098\n",
            "Epoch: 170, Average Loss: 0.4088\n",
            "Epoch: 171, Average Loss: 0.4034\n",
            "Epoch: 172, Average Loss: 0.4103\n",
            "Epoch: 173, Average Loss: 0.4081\n",
            "Epoch: 174, Average Loss: 0.4039\n",
            "Epoch: 175, Average Loss: 0.4130\n",
            "Epoch: 176, Average Loss: 0.4054\n",
            "Epoch: 177, Average Loss: 0.4057\n",
            "Epoch: 178, Average Loss: 0.4090\n",
            "Epoch: 179, Average Loss: 0.4052\n",
            "Epoch: 180, Average Loss: 0.4030\n",
            "Evaluating........................................................................................................................epoch:180, time: 496.856673(s), valid (NDCG@10: 0.6207, HR@10: 0.8483), test (NDCG@10: 0.5897, HR@10: 0.8240)\n",
            "Epoch: 181, Average Loss: 0.4091\n",
            "Epoch: 182, Average Loss: 0.4058\n",
            "Epoch: 183, Average Loss: 0.4026\n",
            "Epoch: 184, Average Loss: 0.4072\n",
            "Epoch: 185, Average Loss: 0.4052\n",
            "Epoch: 186, Average Loss: 0.4040\n",
            "Epoch: 187, Average Loss: 0.4065\n",
            "Epoch: 188, Average Loss: 0.4045\n",
            "Epoch: 189, Average Loss: 0.4012\n",
            "Epoch: 190, Average Loss: 0.4091\n",
            "Epoch: 191, Average Loss: 0.4036\n",
            "Epoch: 192, Average Loss: 0.4047\n",
            "Epoch: 193, Average Loss: 0.4084\n",
            "Epoch: 194, Average Loss: 0.4026\n",
            "Epoch: 195, Average Loss: 0.4023\n",
            "Epoch: 196, Average Loss: 0.4085\n",
            "Epoch: 197, Average Loss: 0.4018\n",
            "Epoch: 198, Average Loss: 0.4019\n",
            "Epoch: 199, Average Loss: 0.4072\n",
            "Epoch: 200, Average Loss: 0.4041\n",
            "Evaluating........................................................................................................................epoch:200, time: 550.098272(s), valid (NDCG@10: 0.6177, HR@10: 0.8482), test (NDCG@10: 0.5863, HR@10: 0.8200)\n",
            "Epoch: 201, Average Loss: 0.4023\n",
            "Done training on ml-1m\n",
            "\n",
            "\n",
            "============================================================\n",
            "RUNNING SASREC ON: AMAZON-BOOK\n",
            "============================================================\n",
            "\n",
            "Dataset 'amazon-book' already prepared.\n",
            "average sequence length: 54.69\n",
            "Epoch: 1, Average Loss: 1.1923\n",
            "Epoch: 2, Average Loss: 0.9373\n",
            "Epoch: 3, Average Loss: 0.7754\n",
            "Epoch: 4, Average Loss: 0.6654\n",
            "Epoch: 5, Average Loss: 0.5847\n",
            "Epoch: 6, Average Loss: 0.5316\n",
            "Epoch: 7, Average Loss: 0.4884\n",
            "Epoch: 8, Average Loss: 0.4520\n",
            "Epoch: 9, Average Loss: 0.4271\n",
            "Epoch: 10, Average Loss: 0.4017\n",
            "Epoch: 11, Average Loss: 0.3824\n",
            "Epoch: 12, Average Loss: 0.3697\n",
            "Epoch: 13, Average Loss: 0.3549\n",
            "Epoch: 14, Average Loss: 0.3436\n",
            "Epoch: 15, Average Loss: 0.3370\n",
            "Epoch: 16, Average Loss: 0.3253\n",
            "Epoch: 17, Average Loss: 0.3184\n",
            "Epoch: 18, Average Loss: 0.3131\n",
            "Epoch: 19, Average Loss: 0.3052\n",
            "Epoch: 20, Average Loss: 0.3006\n",
            "Evaluating........................................................................................................................................................................................................epoch:20, time: 188.345634(s), valid (NDCG@10: 0.6391, HR@10: 0.8650), test (NDCG@10: 0.6089, HR@10: 0.8445)\n",
            "Epoch: 21, Average Loss: 0.2974\n",
            "Epoch: 22, Average Loss: 0.2913\n",
            "Epoch: 23, Average Loss: 0.2862\n",
            "Epoch: 24, Average Loss: 0.2841\n",
            "Epoch: 25, Average Loss: 0.2793\n",
            "Epoch: 26, Average Loss: 0.2766\n",
            "Epoch: 27, Average Loss: 0.2762\n",
            "Epoch: 28, Average Loss: 0.2706\n",
            "Epoch: 29, Average Loss: 0.2688\n",
            "Epoch: 30, Average Loss: 0.2676\n",
            "Epoch: 31, Average Loss: 0.2651\n",
            "Epoch: 32, Average Loss: 0.2613\n",
            "Epoch: 33, Average Loss: 0.2617\n",
            "Epoch: 34, Average Loss: 0.2587\n",
            "Epoch: 35, Average Loss: 0.2575\n",
            "Epoch: 36, Average Loss: 0.2571\n",
            "Epoch: 37, Average Loss: 0.2549\n",
            "Epoch: 38, Average Loss: 0.2520\n",
            "Epoch: 39, Average Loss: 0.2524\n",
            "Epoch: 40, Average Loss: 0.2509\n",
            "Evaluating........................................................................................................................................................................................................epoch:40, time: 375.259360(s), valid (NDCG@10: 0.6510, HR@10: 0.8681), test (NDCG@10: 0.6284, HR@10: 0.8461)\n",
            "Epoch: 41, Average Loss: 0.2489\n",
            "Epoch: 42, Average Loss: 0.2488\n",
            "Epoch: 43, Average Loss: 0.2482\n",
            "Epoch: 44, Average Loss: 0.2462\n",
            "Epoch: 45, Average Loss: 0.2461\n",
            "Epoch: 46, Average Loss: 0.2448\n",
            "Epoch: 47, Average Loss: 0.2425\n",
            "Epoch: 48, Average Loss: 0.2435\n",
            "Epoch: 49, Average Loss: 0.2421\n",
            "Epoch: 50, Average Loss: 0.2410\n",
            "Epoch: 51, Average Loss: 0.2414\n",
            "Epoch: 52, Average Loss: 0.2399\n",
            "Epoch: 53, Average Loss: 0.2397\n",
            "Epoch: 54, Average Loss: 0.2390\n",
            "Epoch: 55, Average Loss: 0.2372\n",
            "Epoch: 56, Average Loss: 0.2369\n",
            "Epoch: 57, Average Loss: 0.2367\n",
            "Epoch: 58, Average Loss: 0.2363\n",
            "Epoch: 59, Average Loss: 0.2344\n",
            "Epoch: 60, Average Loss: 0.2347\n",
            "Evaluating........................................................................................................................................................................................................epoch:60, time: 563.278244(s), valid (NDCG@10: 0.6481, HR@10: 0.8627), test (NDCG@10: 0.6298, HR@10: 0.8486)\n",
            "Epoch: 61, Average Loss: 0.2338\n",
            "Epoch: 62, Average Loss: 0.2328\n",
            "Epoch: 63, Average Loss: 0.2336\n",
            "Epoch: 64, Average Loss: 0.2326\n",
            "Epoch: 65, Average Loss: 0.2314\n",
            "Epoch: 66, Average Loss: 0.2317\n",
            "Epoch: 67, Average Loss: 0.2300\n",
            "Epoch: 68, Average Loss: 0.2294\n",
            "Epoch: 69, Average Loss: 0.2303\n",
            "Epoch: 70, Average Loss: 0.2299\n",
            "Epoch: 71, Average Loss: 0.2285\n",
            "Epoch: 72, Average Loss: 0.2280\n",
            "Epoch: 73, Average Loss: 0.2280\n",
            "Epoch: 74, Average Loss: 0.2273\n",
            "Epoch: 75, Average Loss: 0.2278\n",
            "Epoch: 76, Average Loss: 0.2261\n",
            "Epoch: 77, Average Loss: 0.2252\n",
            "Epoch: 78, Average Loss: 0.2266\n",
            "Epoch: 79, Average Loss: 0.2254\n",
            "Epoch: 80, Average Loss: 0.2247\n",
            "Evaluating........................................................................................................................................................................................................epoch:80, time: 751.061462(s), valid (NDCG@10: 0.6520, HR@10: 0.8620), test (NDCG@10: 0.6252, HR@10: 0.8451)\n",
            "Epoch: 81, Average Loss: 0.2249\n",
            "Epoch: 82, Average Loss: 0.2243\n",
            "Epoch: 83, Average Loss: 0.2226\n",
            "Epoch: 84, Average Loss: 0.2234\n",
            "Epoch: 85, Average Loss: 0.2224\n",
            "Epoch: 86, Average Loss: 0.2216\n",
            "Epoch: 87, Average Loss: 0.2233\n",
            "Epoch: 88, Average Loss: 0.2214\n",
            "Epoch: 89, Average Loss: 0.2203\n",
            "Epoch: 90, Average Loss: 0.2213\n",
            "Epoch: 91, Average Loss: 0.2207\n",
            "Epoch: 92, Average Loss: 0.2203\n",
            "Epoch: 93, Average Loss: 0.2200\n",
            "Epoch: 94, Average Loss: 0.2195\n",
            "Epoch: 95, Average Loss: 0.2184\n",
            "Epoch: 96, Average Loss: 0.2191\n",
            "Epoch: 97, Average Loss: 0.2194\n",
            "Epoch: 98, Average Loss: 0.2177\n",
            "Epoch: 99, Average Loss: 0.2188\n",
            "Epoch: 100, Average Loss: 0.2185\n",
            "Evaluating........................................................................................................................................................................................................epoch:100, time: 939.210215(s), valid (NDCG@10: 0.6566, HR@10: 0.8650), test (NDCG@10: 0.6293, HR@10: 0.8404)\n",
            "Epoch: 101, Average Loss: 0.2165\n",
            "Epoch: 102, Average Loss: 0.2181\n",
            "Epoch: 103, Average Loss: 0.2177\n",
            "Epoch: 104, Average Loss: 0.2163\n",
            "Epoch: 105, Average Loss: 0.2163\n",
            "Epoch: 106, Average Loss: 0.2170\n",
            "Epoch: 107, Average Loss: 0.2145\n",
            "Epoch: 108, Average Loss: 0.2163\n",
            "Epoch: 109, Average Loss: 0.2159\n",
            "Epoch: 110, Average Loss: 0.2158\n",
            "Epoch: 111, Average Loss: 0.2149\n",
            "Epoch: 112, Average Loss: 0.2150\n",
            "Epoch: 113, Average Loss: 0.2134\n",
            "Epoch: 114, Average Loss: 0.2154\n",
            "Epoch: 115, Average Loss: 0.2138\n",
            "Epoch: 116, Average Loss: 0.2130\n",
            "Epoch: 117, Average Loss: 0.2137\n",
            "Epoch: 118, Average Loss: 0.2137\n",
            "Epoch: 119, Average Loss: 0.2133\n",
            "Epoch: 120, Average Loss: 0.2138\n",
            "Evaluating........................................................................................................................................................................................................epoch:120, time: 1124.873259(s), valid (NDCG@10: 0.6517, HR@10: 0.8596), test (NDCG@10: 0.6315, HR@10: 0.8498)\n",
            "Epoch: 121, Average Loss: 0.2128\n",
            "Epoch: 122, Average Loss: 0.2117\n",
            "Epoch: 123, Average Loss: 0.2118\n",
            "Epoch: 124, Average Loss: 0.2119\n",
            "Epoch: 125, Average Loss: 0.2120\n",
            "Epoch: 126, Average Loss: 0.2120\n",
            "Epoch: 127, Average Loss: 0.2121\n",
            "Epoch: 128, Average Loss: 0.2107\n",
            "Epoch: 129, Average Loss: 0.2116\n",
            "Epoch: 130, Average Loss: 0.2110\n",
            "Epoch: 131, Average Loss: 0.2106\n",
            "Epoch: 132, Average Loss: 0.2115\n",
            "Epoch: 133, Average Loss: 0.2098\n",
            "Epoch: 134, Average Loss: 0.2095\n",
            "Epoch: 135, Average Loss: 0.2101\n",
            "Epoch: 136, Average Loss: 0.2097\n",
            "Epoch: 137, Average Loss: 0.2094\n",
            "Epoch: 138, Average Loss: 0.2096\n",
            "Epoch: 139, Average Loss: 0.2094\n",
            "Epoch: 140, Average Loss: 0.2095\n",
            "Evaluating........................................................................................................................................................................................................epoch:140, time: 1310.963890(s), valid (NDCG@10: 0.6562, HR@10: 0.8650), test (NDCG@10: 0.6359, HR@10: 0.8468)\n",
            "Epoch: 141, Average Loss: 0.2091\n",
            "Epoch: 142, Average Loss: 0.2087\n",
            "Epoch: 143, Average Loss: 0.2080\n",
            "Epoch: 144, Average Loss: 0.2087\n",
            "Epoch: 145, Average Loss: 0.2086\n",
            "Epoch: 146, Average Loss: 0.2084\n",
            "Epoch: 147, Average Loss: 0.2091\n",
            "Epoch: 148, Average Loss: 0.2070\n",
            "Epoch: 149, Average Loss: 0.2082\n",
            "Epoch: 150, Average Loss: 0.2073\n",
            "Epoch: 151, Average Loss: 0.2085\n",
            "Epoch: 152, Average Loss: 0.2067\n",
            "Epoch: 153, Average Loss: 0.2071\n",
            "Epoch: 154, Average Loss: 0.2071\n",
            "Epoch: 155, Average Loss: 0.2057\n",
            "Epoch: 156, Average Loss: 0.2065\n",
            "Epoch: 157, Average Loss: 0.2064\n",
            "Epoch: 158, Average Loss: 0.2060\n",
            "Epoch: 159, Average Loss: 0.2056\n",
            "Epoch: 160, Average Loss: 0.2066\n",
            "Evaluating........................................................................................................................................................................................................epoch:160, time: 1496.985898(s), valid (NDCG@10: 0.6529, HR@10: 0.8625), test (NDCG@10: 0.6235, HR@10: 0.8359)\n",
            "Epoch: 161, Average Loss: 0.2055\n",
            "Epoch: 162, Average Loss: 0.2059\n",
            "Epoch: 163, Average Loss: 0.2053\n",
            "Epoch: 164, Average Loss: 0.2050\n",
            "Epoch: 165, Average Loss: 0.2063\n",
            "Epoch: 166, Average Loss: 0.2048\n",
            "Epoch: 167, Average Loss: 0.2055\n",
            "Epoch: 168, Average Loss: 0.2052\n",
            "Epoch: 169, Average Loss: 0.2054\n",
            "Epoch: 170, Average Loss: 0.2048\n",
            "Epoch: 171, Average Loss: 0.2050\n",
            "Epoch: 172, Average Loss: 0.2041\n",
            "Epoch: 173, Average Loss: 0.2035\n",
            "Epoch: 174, Average Loss: 0.2043\n",
            "Epoch: 175, Average Loss: 0.2031\n",
            "Epoch: 176, Average Loss: 0.2037\n",
            "Epoch: 177, Average Loss: 0.2041\n",
            "Epoch: 178, Average Loss: 0.2037\n",
            "Epoch: 179, Average Loss: 0.2033\n",
            "Epoch: 180, Average Loss: 0.2042\n",
            "Evaluating........................................................................................................................................................................................................epoch:180, time: 1683.754994(s), valid (NDCG@10: 0.6576, HR@10: 0.8655), test (NDCG@10: 0.6377, HR@10: 0.8481)\n",
            "Epoch: 181, Average Loss: 0.2030\n",
            "Epoch: 182, Average Loss: 0.2045\n",
            "Epoch: 183, Average Loss: 0.2030\n",
            "Epoch: 184, Average Loss: 0.2030\n",
            "Epoch: 185, Average Loss: 0.2031\n",
            "Epoch: 186, Average Loss: 0.2036\n",
            "Epoch: 187, Average Loss: 0.2030\n",
            "Epoch: 188, Average Loss: 0.2024\n",
            "Epoch: 189, Average Loss: 0.2029\n",
            "Epoch: 190, Average Loss: 0.2025\n",
            "Epoch: 191, Average Loss: 0.2027\n",
            "Epoch: 192, Average Loss: 0.2032\n",
            "Epoch: 193, Average Loss: 0.2022\n",
            "Epoch: 194, Average Loss: 0.2023\n",
            "Epoch: 195, Average Loss: 0.2033\n",
            "Epoch: 196, Average Loss: 0.2024\n",
            "Epoch: 197, Average Loss: 0.2019\n",
            "Epoch: 198, Average Loss: 0.2015\n",
            "Epoch: 199, Average Loss: 0.2030\n",
            "Epoch: 200, Average Loss: 0.2014\n",
            "Evaluating........................................................................................................................................................................................................epoch:200, time: 1871.258722(s), valid (NDCG@10: 0.6494, HR@10: 0.8567), test (NDCG@10: 0.6352, HR@10: 0.8472)\n",
            "Epoch: 201, Average Loss: 0.2020\n",
            "Done training on amazon-book\n",
            "\n",
            "\n",
            "============================================================\n",
            "RUNNING SASREC ON: YELP2018\n",
            "============================================================\n",
            "\n",
            "Dataset 'yelp2018' already prepared.\n",
            "average sequence length: 47.31\n",
            "Epoch: 1, Average Loss: 1.1899\n",
            "Epoch: 2, Average Loss: 0.9423\n",
            "Epoch: 3, Average Loss: 0.7899\n",
            "Epoch: 4, Average Loss: 0.6615\n",
            "Epoch: 5, Average Loss: 0.6036\n",
            "Epoch: 6, Average Loss: 0.5648\n",
            "Epoch: 7, Average Loss: 0.5285\n",
            "Epoch: 8, Average Loss: 0.4976\n",
            "Epoch: 9, Average Loss: 0.4697\n",
            "Epoch: 10, Average Loss: 0.4484\n",
            "Epoch: 11, Average Loss: 0.4333\n",
            "Epoch: 12, Average Loss: 0.4224\n",
            "Epoch: 13, Average Loss: 0.4096\n",
            "Epoch: 14, Average Loss: 0.4010\n",
            "Epoch: 15, Average Loss: 0.3946\n",
            "Epoch: 16, Average Loss: 0.3882\n",
            "Epoch: 17, Average Loss: 0.3816\n",
            "Epoch: 18, Average Loss: 0.3781\n",
            "Epoch: 19, Average Loss: 0.3715\n",
            "Epoch: 20, Average Loss: 0.3674\n",
            "Evaluating........................................................................................................................................................................................................epoch:20, time: 108.139684(s), valid (NDCG@10: 0.5795, HR@10: 0.8801), test (NDCG@10: 0.5672, HR@10: 0.8670)\n",
            "Epoch: 21, Average Loss: 0.3633\n",
            "Epoch: 22, Average Loss: 0.3606\n",
            "Epoch: 23, Average Loss: 0.3579\n",
            "Epoch: 24, Average Loss: 0.3520\n",
            "Epoch: 25, Average Loss: 0.3499\n",
            "Epoch: 26, Average Loss: 0.3464\n",
            "Epoch: 27, Average Loss: 0.3438\n",
            "Epoch: 28, Average Loss: 0.3402\n",
            "Epoch: 29, Average Loss: 0.3376\n",
            "Epoch: 30, Average Loss: 0.3371\n",
            "Epoch: 31, Average Loss: 0.3335\n",
            "Epoch: 32, Average Loss: 0.3318\n",
            "Epoch: 33, Average Loss: 0.3294\n",
            "Epoch: 34, Average Loss: 0.3277\n",
            "Epoch: 35, Average Loss: 0.3250\n",
            "Epoch: 36, Average Loss: 0.3262\n",
            "Epoch: 37, Average Loss: 0.3230\n",
            "Epoch: 38, Average Loss: 0.3208\n",
            "Epoch: 39, Average Loss: 0.3206\n",
            "Epoch: 40, Average Loss: 0.3183\n",
            "Evaluating........................................................................................................................................................................................................epoch:40, time: 216.852876(s), valid (NDCG@10: 0.5972, HR@10: 0.8869), test (NDCG@10: 0.5822, HR@10: 0.8737)\n",
            "Epoch: 41, Average Loss: 0.3155\n",
            "Epoch: 42, Average Loss: 0.3146\n",
            "Epoch: 43, Average Loss: 0.3134\n",
            "Epoch: 44, Average Loss: 0.3141\n",
            "Epoch: 45, Average Loss: 0.3126\n",
            "Epoch: 46, Average Loss: 0.3121\n",
            "Epoch: 47, Average Loss: 0.3096\n",
            "Epoch: 48, Average Loss: 0.3090\n",
            "Epoch: 49, Average Loss: 0.3073\n",
            "Epoch: 50, Average Loss: 0.3076\n",
            "Epoch: 51, Average Loss: 0.3061\n",
            "Epoch: 52, Average Loss: 0.3038\n",
            "Epoch: 53, Average Loss: 0.3042\n",
            "Epoch: 54, Average Loss: 0.3031\n",
            "Epoch: 55, Average Loss: 0.3025\n",
            "Epoch: 56, Average Loss: 0.3005\n",
            "Epoch: 57, Average Loss: 0.3006\n",
            "Epoch: 58, Average Loss: 0.2981\n",
            "Epoch: 59, Average Loss: 0.2992\n",
            "Epoch: 60, Average Loss: 0.2992\n",
            "Evaluating........................................................................................................................................................................................................epoch:60, time: 325.074364(s), valid (NDCG@10: 0.5974, HR@10: 0.8829), test (NDCG@10: 0.5836, HR@10: 0.8702)\n",
            "Epoch: 61, Average Loss: 0.2967\n",
            "Epoch: 62, Average Loss: 0.2973\n",
            "Epoch: 63, Average Loss: 0.2950\n",
            "Epoch: 64, Average Loss: 0.2954\n",
            "Epoch: 65, Average Loss: 0.2955\n",
            "Epoch: 66, Average Loss: 0.2947\n",
            "Epoch: 67, Average Loss: 0.2928\n",
            "Epoch: 68, Average Loss: 0.2920\n",
            "Epoch: 69, Average Loss: 0.2919\n",
            "Epoch: 70, Average Loss: 0.2899\n",
            "Epoch: 71, Average Loss: 0.2924\n",
            "Epoch: 72, Average Loss: 0.2890\n",
            "Epoch: 73, Average Loss: 0.2887\n",
            "Epoch: 74, Average Loss: 0.2890\n",
            "Epoch: 75, Average Loss: 0.2886\n",
            "Epoch: 76, Average Loss: 0.2867\n",
            "Epoch: 77, Average Loss: 0.2866\n",
            "Epoch: 78, Average Loss: 0.2865\n",
            "Epoch: 79, Average Loss: 0.2838\n",
            "Epoch: 80, Average Loss: 0.2849\n",
            "Evaluating........................................................................................................................................................................................................epoch:80, time: 433.802016(s), valid (NDCG@10: 0.5999, HR@10: 0.8865), test (NDCG@10: 0.5845, HR@10: 0.8701)\n",
            "Epoch: 81, Average Loss: 0.2843\n",
            "Epoch: 82, Average Loss: 0.2835\n",
            "Epoch: 83, Average Loss: 0.2850\n",
            "Epoch: 84, Average Loss: 0.2837\n",
            "Epoch: 85, Average Loss: 0.2827\n",
            "Epoch: 86, Average Loss: 0.2812\n",
            "Epoch: 87, Average Loss: 0.2819\n",
            "Epoch: 88, Average Loss: 0.2824\n",
            "Epoch: 89, Average Loss: 0.2811\n",
            "Epoch: 90, Average Loss: 0.2818\n",
            "Epoch: 91, Average Loss: 0.2804\n",
            "Epoch: 92, Average Loss: 0.2805\n",
            "Epoch: 93, Average Loss: 0.2796\n",
            "Epoch: 94, Average Loss: 0.2776\n",
            "Epoch: 95, Average Loss: 0.2791\n",
            "Epoch: 96, Average Loss: 0.2769\n",
            "Epoch: 97, Average Loss: 0.2787\n",
            "Epoch: 98, Average Loss: 0.2783\n",
            "Epoch: 99, Average Loss: 0.2780\n",
            "Epoch: 100, Average Loss: 0.2747\n",
            "Evaluating........................................................................................................................................................................................................epoch:100, time: 542.315223(s), valid (NDCG@10: 0.5993, HR@10: 0.8848), test (NDCG@10: 0.5840, HR@10: 0.8660)\n",
            "Epoch: 101, Average Loss: 0.2751\n",
            "Epoch: 102, Average Loss: 0.2778\n",
            "Epoch: 103, Average Loss: 0.2748\n",
            "Epoch: 104, Average Loss: 0.2760\n",
            "Epoch: 105, Average Loss: 0.2745\n",
            "Epoch: 106, Average Loss: 0.2754\n",
            "Epoch: 107, Average Loss: 0.2745\n",
            "Epoch: 108, Average Loss: 0.2741\n",
            "Epoch: 109, Average Loss: 0.2727\n",
            "Epoch: 110, Average Loss: 0.2731\n",
            "Epoch: 111, Average Loss: 0.2733\n",
            "Epoch: 112, Average Loss: 0.2718\n",
            "Epoch: 113, Average Loss: 0.2724\n",
            "Epoch: 114, Average Loss: 0.2719\n",
            "Epoch: 115, Average Loss: 0.2715\n",
            "Epoch: 116, Average Loss: 0.2719\n",
            "Epoch: 117, Average Loss: 0.2705\n",
            "Epoch: 118, Average Loss: 0.2698\n",
            "Epoch: 119, Average Loss: 0.2688\n",
            "Epoch: 120, Average Loss: 0.2703\n",
            "Evaluating........................................................................................................................................................................................................epoch:120, time: 651.862923(s), valid (NDCG@10: 0.5934, HR@10: 0.8716), test (NDCG@10: 0.5845, HR@10: 0.8680)\n",
            "Epoch: 121, Average Loss: 0.2707\n",
            "Epoch: 122, Average Loss: 0.2681\n",
            "Epoch: 123, Average Loss: 0.2691\n",
            "Epoch: 124, Average Loss: 0.2692\n",
            "Epoch: 125, Average Loss: 0.2685\n",
            "Epoch: 126, Average Loss: 0.2694\n",
            "Epoch: 127, Average Loss: 0.2672\n",
            "Epoch: 128, Average Loss: 0.2681\n",
            "Epoch: 129, Average Loss: 0.2665\n",
            "Epoch: 130, Average Loss: 0.2662\n",
            "Epoch: 131, Average Loss: 0.2663\n",
            "Epoch: 132, Average Loss: 0.2675\n",
            "Epoch: 133, Average Loss: 0.2649\n",
            "Epoch: 134, Average Loss: 0.2646\n",
            "Epoch: 135, Average Loss: 0.2656\n",
            "Epoch: 136, Average Loss: 0.2655\n",
            "Epoch: 137, Average Loss: 0.2664\n",
            "Epoch: 138, Average Loss: 0.2655\n",
            "Epoch: 139, Average Loss: 0.2648\n",
            "Epoch: 140, Average Loss: 0.2636\n",
            "Evaluating........................................................................................................................................................................................................epoch:140, time: 760.334469(s), valid (NDCG@10: 0.6004, HR@10: 0.8788), test (NDCG@10: 0.5899, HR@10: 0.8681)\n",
            "Epoch: 141, Average Loss: 0.2639\n",
            "Epoch: 142, Average Loss: 0.2636\n",
            "Epoch: 143, Average Loss: 0.2646\n",
            "Epoch: 144, Average Loss: 0.2636\n",
            "Epoch: 145, Average Loss: 0.2624\n",
            "Epoch: 146, Average Loss: 0.2632\n",
            "Epoch: 147, Average Loss: 0.2618\n",
            "Epoch: 148, Average Loss: 0.2627\n",
            "Epoch: 149, Average Loss: 0.2622\n",
            "Epoch: 150, Average Loss: 0.2622\n",
            "Epoch: 151, Average Loss: 0.2633\n",
            "Epoch: 152, Average Loss: 0.2626\n",
            "Epoch: 153, Average Loss: 0.2627\n",
            "Epoch: 154, Average Loss: 0.2611\n",
            "Epoch: 155, Average Loss: 0.2616\n",
            "Epoch: 156, Average Loss: 0.2602\n",
            "Epoch: 157, Average Loss: 0.2609\n",
            "Epoch: 158, Average Loss: 0.2609\n",
            "Epoch: 159, Average Loss: 0.2602\n",
            "Epoch: 160, Average Loss: 0.2610\n",
            "Evaluating........................................................................................................................................................................................................epoch:160, time: 871.083554(s), valid (NDCG@10: 0.5958, HR@10: 0.8771), test (NDCG@10: 0.5918, HR@10: 0.8649)\n",
            "Epoch: 161, Average Loss: 0.2608\n",
            "Epoch: 162, Average Loss: 0.2588\n",
            "Epoch: 163, Average Loss: 0.2599\n",
            "Epoch: 164, Average Loss: 0.2614\n",
            "Epoch: 165, Average Loss: 0.2598\n",
            "Epoch: 166, Average Loss: 0.2594\n",
            "Epoch: 167, Average Loss: 0.2583\n",
            "Epoch: 168, Average Loss: 0.2573\n",
            "Epoch: 169, Average Loss: 0.2584\n",
            "Epoch: 170, Average Loss: 0.2578\n",
            "Epoch: 171, Average Loss: 0.2580\n",
            "Epoch: 172, Average Loss: 0.2592\n",
            "Epoch: 173, Average Loss: 0.2568\n",
            "Epoch: 174, Average Loss: 0.2589\n",
            "Epoch: 175, Average Loss: 0.2579\n",
            "Epoch: 176, Average Loss: 0.2583\n",
            "Epoch: 177, Average Loss: 0.2569\n",
            "Epoch: 178, Average Loss: 0.2581\n",
            "Epoch: 179, Average Loss: 0.2561\n",
            "Epoch: 180, Average Loss: 0.2584\n",
            "Evaluating........................................................................................................................................................................................................epoch:180, time: 980.137933(s), valid (NDCG@10: 0.5949, HR@10: 0.8761), test (NDCG@10: 0.5853, HR@10: 0.8651)\n",
            "Epoch: 181, Average Loss: 0.2557\n",
            "Epoch: 182, Average Loss: 0.2562\n",
            "Epoch: 183, Average Loss: 0.2569\n",
            "Epoch: 184, Average Loss: 0.2552\n",
            "Epoch: 185, Average Loss: 0.2553\n",
            "Epoch: 186, Average Loss: 0.2555\n",
            "Epoch: 187, Average Loss: 0.2558\n",
            "Epoch: 188, Average Loss: 0.2559\n",
            "Epoch: 189, Average Loss: 0.2553\n",
            "Epoch: 190, Average Loss: 0.2551\n",
            "Epoch: 191, Average Loss: 0.2559\n",
            "Epoch: 192, Average Loss: 0.2550\n",
            "Epoch: 193, Average Loss: 0.2535\n",
            "Epoch: 194, Average Loss: 0.2547\n",
            "Epoch: 195, Average Loss: 0.2557\n",
            "Epoch: 196, Average Loss: 0.2536\n",
            "Epoch: 197, Average Loss: 0.2558\n",
            "Epoch: 198, Average Loss: 0.2550\n",
            "Epoch: 199, Average Loss: 0.2544\n",
            "Epoch: 200, Average Loss: 0.2544\n",
            "Evaluating........................................................................................................................................................................................................epoch:200, time: 1087.723364(s), valid (NDCG@10: 0.5977, HR@10: 0.8765), test (NDCG@10: 0.5872, HR@10: 0.8641)\n",
            "Epoch: 201, Average Loss: 0.2547\n",
            "Done training on yelp2018\n",
            "\n",
            "\n",
            "All dataset runs completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SASRecAnalyzer:\n",
        "    def __init__(self, model, item_num):\n",
        "        self.model = model.to(torch.device('cpu')) # Move to CPU for analysis\n",
        "        self.item_num = item_num\n",
        "        self.item_embeddings = self.model.item_emb.weight.data.numpy()[1:] # Exclude padding item 0\n",
        "\n",
        "    def _calculate_metrics(self, high_dim_embeds, low_dim_embeds, k=10):\n",
        "        # This helper function is the same as the one used for KGAT\n",
        "        nn_high = NearestNeighbors(n_neighbors=k+1).fit(high_dim_embeds)\n",
        "        high_dim_neighbors = nn_high.kneighbors(high_dim_embeds, return_distance=False)[:, 1:]\n",
        "        nn_low = NearestNeighbors(n_neighbors=k+1).fit(low_dim_embeds)\n",
        "        low_dim_neighbors = nn_low.kneighbors(low_dim_embeds, return_distance=False)[:, 1:]\n",
        "        np_scores = [len(set(high_dim_neighbors[i]) & set(low_dim_neighbors[i])) / k for i in range(len(high_dim_embeds))]\n",
        "        return np.mean(np_scores)\n",
        "\n",
        "    def plot_tsne_and_metrics(self, n_samples=1000, k=10, n_clusters=10):\n",
        "        \"\"\"\n",
        "        Generates and plots a t-SNE visualization of item embeddings and calculates metrics.\n",
        "        \"\"\"\n",
        "        print(f\"Running t-SNE on a sample of {n_samples} item embeddings...\")\n",
        "\n",
        "        # Sample a subset of items for efficient analysis\n",
        "        sample_indices = np.random.choice(self.item_embeddings.shape[0], min(n_samples, self.item_embeddings.shape[0]), replace=False)\n",
        "        sampled_embeds_high_dim = self.item_embeddings[sample_indices]\n",
        "\n",
        "        # Perform t-SNE\n",
        "        tsne = TSNE(n_components=2, perplexity=40, n_iter=1000, random_state=42)\n",
        "        embeds_low_dim = tsne.fit_transform(sampled_embeds_high_dim)\n",
        "\n",
        "        # Calculate Neighborhood Preservation\n",
        "        np_metric = self._calculate_metrics(sampled_embeds_high_dim, embeds_low_dim, k=k)\n",
        "        print(\"\\n--- Embedding Quality Metrics ---\")\n",
        "        print(f\"Neighborhood Preservation (NP@{k}): {np_metric:.4f}\")\n",
        "        print(\"---------------------------------\\n\")\n",
        "\n",
        "        # Perform K-Means clustering to find item groups\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10).fit(sampled_embeds_high_dim)\n",
        "        cluster_labels = kmeans.labels_\n",
        "\n",
        "        # Plotting\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.scatterplot(\n",
        "            x=embeds_low_dim[:, 0], y=embeds_low_dim[:, 1],\n",
        "            hue=cluster_labels,\n",
        "            palette=sns.color_palette(\"viridis\", n_clusters),\n",
        "            legend='full',\n",
        "            alpha=0.7\n",
        "        )\n",
        "        plt.title(f't-SNE Visualization of SASRec Item Embeddings ({n_clusters} Clusters)')\n",
        "        plt.xlabel('t-SNE Dimension 1')\n",
        "        plt.ylabel('t-SNE Dimension 2')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "ua79mJOFuXsJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sasrec_models():\n",
        "    \"\"\"\n",
        "    Loads trained SASRec models and runs the t-SNE and metrics analysis.\n",
        "    \"\"\"\n",
        "    # Assuming your Args class and data_partition function are available\n",
        "    datasets_to_analyze = ['ml-1m', 'amazon-book', 'yelp2018']\n",
        "\n",
        "    for dataset_name in datasets_to_analyze:\n",
        "        print(f\"\\n\\n{'='*60}\")\n",
        "        print(f\"ANALYZING SASREC MODEL FOR: {dataset_name.upper()}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Setup args and get dataset info\n",
        "        args = Args()\n",
        "        args.dataset = dataset_name\n",
        "        if dataset_name in ['amazon-book', 'yelp2018']:\n",
        "            args.maxlen = 50\n",
        "\n",
        "        _, _, _, usernum, itemnum = data_partition(args.dataset)\n",
        "\n",
        "        # Find the best saved model for this dataset\n",
        "        model_dir = args.dataset + '_' + args.train_dir\n",
        "        if not os.path.isdir(model_dir):\n",
        "            print(f\"Model directory not found for {dataset_name}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # A simple way to find a saved model file - you can make this more specific\n",
        "        saved_models = [f for f in os.listdir(model_dir) if f.endswith('.pth')]\n",
        "        if not saved_models:\n",
        "            print(f\"No saved .pth model found for {dataset_name}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        model_path = os.path.join(model_dir, saved_models[-1]) # Load the last saved model\n",
        "        print(f\"Loading model from: {model_path}\")\n",
        "\n",
        "        # Load model and state\n",
        "        model = SASRec(usernum, itemnum, args)\n",
        "        model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "        model.eval()\n",
        "\n",
        "        # Create analyzer and run the analysis\n",
        "        analyzer = SASRecAnalyzer(model, itemnum)\n",
        "        analyzer.plot_tsne_and_metrics()\n",
        "\n",
        "# After the main training block, call this new function\n",
        "if __name__ == '__main__':\n",
        "    # ... your existing training loop ...\n",
        "\n",
        "    # After all training is done, run the analysis\n",
        "    analyze_sasrec_models()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwZtbGwluYNX",
        "outputId": "18b694cb-90cc-421d-d0ed-89cdcf634a23"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "============================================================\n",
            "ANALYZING SASREC MODEL FOR: ML-1M\n",
            "============================================================\n",
            "No saved .pth model found for ml-1m. Skipping.\n",
            "\n",
            "\n",
            "============================================================\n",
            "ANALYZING SASREC MODEL FOR: AMAZON-BOOK\n",
            "============================================================\n",
            "No saved .pth model found for amazon-book. Skipping.\n",
            "\n",
            "\n",
            "============================================================\n",
            "ANALYZING SASREC MODEL FOR: YELP2018\n",
            "============================================================\n",
            "No saved .pth model found for yelp2018. Skipping.\n"
          ]
        }
      ]
    }
  ]
}