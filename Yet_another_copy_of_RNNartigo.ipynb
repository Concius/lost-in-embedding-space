{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "vfkO2lvV9FNP",
        "outputId": "907fa260-f873-4ceb-bda4-c2a928446341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data directory\n",
        "!mkdir -p data\n",
        "\n",
        "# Download MovieLens-1M dataset from the GitHub repository\n",
        "!wget -O data/ml-1m.txt https://raw.githubusercontent.com/pmixer/SASRec.pytorch/main/python/data/ml-1m.txt\n",
        "\n",
        "print(\"Dataset downloaded successfully!\")\n",
        "\n",
        "# Verify the dataset\n",
        "if os.path.exists('data/ml-1m.txt'):\n",
        "    with open('data/ml-1m.txt', 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    print(f\"Dataset loaded with {len(lines)} interactions\")\n",
        "    print(\"First few lines:\")\n",
        "    for i in range(min(5, len(lines))):\n",
        "        print(lines[i].strip())\n",
        "else:\n",
        "    print(\"Error: Dataset not found!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVeW8vNsQfg7",
        "outputId": "739cc8e7-0941-42e7-e27c-b9a5a0b0b7fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-06 03:21:27--  https://raw.githubusercontent.com/pmixer/SASRec.pytorch/main/python/data/ml-1m.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9053831 (8.6M) [text/plain]\n",
            "Saving to: ‘data/ml-1m.txt’\n",
            "\n",
            "data/ml-1m.txt      100%[===================>]   8.63M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-06-06 03:21:28 (216 MB/s) - ‘data/ml-1m.txt’ saved [9053831/9053831]\n",
            "\n",
            "Dataset downloaded successfully!\n",
            "Dataset loaded with 999611 interactions\n",
            "First few lines:\n",
            "1 1\n",
            "1 2\n",
            "1 3\n",
            "1 4\n",
            "1 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric\n",
        "!pip install wandb\n",
        "!pip install pyg-lib -f https://data.pyg.org/whl/torch-2.6.0+cu124.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIyXolLc9TN3",
        "outputId": "ef638b68-cdc4-4f8d-b5f0-12ff89b1278d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.11)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.29.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Collecting pyg-lib\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/pyg_lib-0.4.0%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyg-lib\n",
            "Successfully installed pyg-lib-0.4.0+pt26cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRQ4006uIsib",
        "outputId": "75d2a912-8d6f-4941-9c77-3a04a2df49e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.1-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.1 colorlog-6.9.0 optuna-4.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
      ],
      "metadata": {
        "id": "awrrES2HTzEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GATConv, MessagePassing\n",
        "from torch_geometric.utils import degree\n",
        "from collections import defaultdict\n",
        "\n",
        "#===============================================================================\n",
        "# 1) Utilities\n",
        "#===============================================================================\n",
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark    = False\n",
        "\n",
        "def create_train_val_test_split(ratings_df,\n",
        "                                val_ratio=0.1,\n",
        "                                test_ratio=0.1,\n",
        "                                min_interactions=5,\n",
        "                                positive_threshold=None,\n",
        "                                seed=42):\n",
        "    \"\"\"\n",
        "    Chronological per-user train/val/test split.\n",
        "    Returns train_df, val_df, test_df, num_users, num_items\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    df = ratings_df.copy()\n",
        "    if positive_threshold is not None:\n",
        "        df = df[df.rating >= positive_threshold]\n",
        "\n",
        "    counts = df.userId.value_counts()\n",
        "    valid  = counts[counts >= min_interactions].index\n",
        "    df = df[df.userId.isin(valid)]\n",
        "    df = df.sort_values(['userId','timestamp'])\n",
        "\n",
        "    trains, vals, tests = [], [], []\n",
        "    for _, udf in df.groupby('userId'):\n",
        "        n = len(udf)\n",
        "        n_test  = max(1, int(n*test_ratio))\n",
        "        n_val   = max(1, int(n*val_ratio))\n",
        "        n_train = n - n_val - n_test\n",
        "        if n_train < 1:\n",
        "            continue\n",
        "        trains.append( udf.iloc[:n_train] )\n",
        "        vals.append(   udf.iloc[n_train:n_train+n_val] )\n",
        "        tests.append(  udf.iloc[n_train+n_val:] )\n",
        "\n",
        "    train_df = pd.concat(trains, ignore_index=True)\n",
        "    val_df   = pd.concat(vals,   ignore_index=True)\n",
        "    test_df  = pd.concat(tests,  ignore_index=True)\n",
        "\n",
        "    # remap to contiguous 0..N-1\n",
        "    u2idx = {u:i for i,u in enumerate(train_df.userId.unique())}\n",
        "    i2idx = {m:i for i,m in enumerate(train_df.movieId.unique())}\n",
        "\n",
        "    for d in (train_df, val_df, test_df):\n",
        "        d['user_idx']  = d.userId .map(u2idx)\n",
        "        d['movie_idx'] = d.movieId.map(i2idx)\n",
        "\n",
        "    num_users = len(u2idx)\n",
        "    num_items = len(i2idx)\n",
        "    print(f\"Split sizes: train={len(train_df)}, val={len(val_df)}, test={len(test_df)}\")\n",
        "    print(f\"#users={num_users}, #items={num_items}\")\n",
        "    return train_df, val_df, test_df, num_users, num_items\n",
        "\n",
        "def create_pyg_data(df, num_users, num_items):\n",
        "    \"\"\"\n",
        "    Build a PyG bipartite graph.\n",
        "    Users: 0..num_users-1, items: num_users..num_users+num_items-1\n",
        "    \"\"\"\n",
        "    us  = torch.LongTensor(df.user_idx.values)\n",
        "    is_ = torch.LongTensor(df.movie_idx.values) + num_users\n",
        "    edge_index = torch.stack([torch.cat([us,is_]), torch.cat([is_,us])], dim=0)\n",
        "    data = Data(edge_index=edge_index, num_nodes=num_users+num_items)\n",
        "    data.num_users = num_users\n",
        "    data.num_items = num_items\n",
        "    data.orig_interactions = torch.stack([us,is_], dim=1)\n",
        "    return data\n",
        "\n",
        "#===============================================================================\n",
        "# 2) Graph eval + train\n",
        "#===============================================================================\n",
        "def evaluate_graph_model(model, train_data, eval_data, device, k=10):\n",
        "    \"\"\"\n",
        "    HR@k, NDCG@k, precision, recall, f1, mrr for graph-based models\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    metrics = defaultdict(list)\n",
        "\n",
        "    # 1) compute embeddings on the correct device\n",
        "    with torch.no_grad():\n",
        "        edge_index = train_data.edge_index.to(device)\n",
        "        embs = model(edge_index)\n",
        "        U = model.num_users\n",
        "        user_embs = embs[:U]\n",
        "        item_embs = embs[U:]\n",
        "\n",
        "    # 2) build train‐map and eval‐map on CPU\n",
        "    train_map = defaultdict(set)\n",
        "    for u,i in train_data.orig_interactions.cpu().tolist():\n",
        "        train_map[u].add(i - U)\n",
        "\n",
        "    eval_map = defaultdict(list)\n",
        "    for u,i in eval_data.orig_interactions.cpu().tolist():\n",
        "        eval_map[u].append(i - U)\n",
        "\n",
        "    # 3) per‐user ranking\n",
        "    for u, true_items in eval_map.items():\n",
        "        if not true_items:\n",
        "            continue\n",
        "\n",
        "        # Perform multiplication on the GPU, then move result to CPU\n",
        "        scores = (item_embs @ user_embs[u]).cpu().numpy()\n",
        "\n",
        "        # mask training items\n",
        "        for ti in train_map[u]:\n",
        "            scores[ti] = -1e9\n",
        "\n",
        "        # top‐k\n",
        "        topk = np.argpartition(-scores, k)[:k]\n",
        "        hits = [1 if t in true_items else 0 for t in topk]\n",
        "\n",
        "        # HR\n",
        "        hr = 1.0 if any(hits) else 0.0\n",
        "        # NDCG\n",
        "        dcg  = sum(h / math.log2(idx+2) for idx,h in enumerate(hits))\n",
        "        idcg = sum(1.0/math.log2(i+2) for i in range(min(len(true_items), k)))\n",
        "        ndcg = dcg/idcg if idcg>0 else 0.0\n",
        "        # prec/rec/f1\n",
        "        prec = sum(hits)/k\n",
        "        rec  = sum(hits)/len(true_items)\n",
        "        f1   = 2*prec*rec/(prec+rec) if (prec+rec)>0 else 0.0\n",
        "        # MRR\n",
        "        mrr  = next((1.0/(idx+1) for idx,h in enumerate(hits) if h), 0.0)\n",
        "\n",
        "        metrics['hr'].append(hr)\n",
        "        metrics['ndcg'].append(ndcg)\n",
        "        metrics['precision'].append(prec)\n",
        "        metrics['recall'].append(rec)\n",
        "        metrics['f1'].append(f1)\n",
        "        metrics['mrr'].append(mrr)\n",
        "\n",
        "    return {m: np.mean(v) for m,v in metrics.items()}\n",
        "\n",
        "def train_graph_epoch_regularized(model,\n",
        "                                  train_data,\n",
        "                                  optimizer,\n",
        "                                  device,\n",
        "                                  batch_size=8192,\n",
        "                                  l2_reg=1e-4):\n",
        "    \"\"\"\n",
        "    One epoch of BPR + L2 training for graph models.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    edge_index = train_data.edge_index.to(device)\n",
        "    pos_edges  = train_data.orig_interactions.to(device)\n",
        "    E = pos_edges.size(0)\n",
        "    perm = torch.randperm(E, device=device)\n",
        "    n_batches = (E + batch_size - 1) // batch_size\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for b in range(n_batches):\n",
        "        idx   = perm[b*batch_size:(b+1)*batch_size]\n",
        "        users = pos_edges[idx,0]\n",
        "        items = pos_edges[idx,1]\n",
        "        negs  = torch.randint(model.num_users,\n",
        "                              model.num_users+model.num_items,\n",
        "                              size=users.size(), device=device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pos_scores = model.predict(edge_index, users, items)\n",
        "        neg_scores = model.predict(edge_index, users, negs)\n",
        "        bpr_loss = -torch.log(torch.sigmoid(pos_scores - neg_scores)+1e-8).mean()\n",
        "        l2_loss  = l2_reg * (\n",
        "                model.user_emb(users).pow(2).mean()\n",
        "              + model.item_emb(items-model.num_users).pow(2).mean()\n",
        "              + model.item_emb(negs-model.num_users).pow(2).mean()\n",
        "        )\n",
        "        loss = bpr_loss + l2_loss\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / n_batches\n",
        "\n",
        "#===============================================================================\n",
        "# 3) Graph Models\n",
        "#===============================================================================\n",
        "class GATRec(nn.Module):\n",
        "    def __init__(self, num_users, num_items,\n",
        "                 embedding_dim=64, hidden_dim=64,\n",
        "                 heads=4, n_layers=3):\n",
        "        super().__init__()\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.user_emb  = nn.Embedding(num_users, embedding_dim)\n",
        "        self.item_emb  = nn.Embedding(num_items, embedding_dim)\n",
        "        self.convs     = nn.ModuleList()\n",
        "        # first layer\n",
        "        self.convs.append(GATConv(embedding_dim, hidden_dim, heads=heads, dropout=0.6))\n",
        "        # middle\n",
        "        for _ in range(n_layers-2):\n",
        "            self.convs.append(GATConv(hidden_dim*heads, hidden_dim, heads=heads, dropout=0.6))\n",
        "        # final\n",
        "        self.convs.append(GATConv(hidden_dim*heads, hidden_dim, heads=1, concat=False, dropout=0.6))\n",
        "        self.dropout = nn.Dropout(0.6)\n",
        "        # init\n",
        "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
        "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
        "\n",
        "    def forward(self, edge_index):\n",
        "        x = torch.cat([self.user_emb.weight, self.item_emb.weight], dim=0)\n",
        "        for conv in self.convs[:-1]:\n",
        "            x = conv(x, edge_index)\n",
        "            x = F.relu(x); x = self.dropout(x)\n",
        "        x = self.convs[-1](x, edge_index)\n",
        "        return x\n",
        "\n",
        "    def predict(self, edge_index, u, i):\n",
        "        embs = self(edge_index)\n",
        "        return (embs[u]*embs[i]).sum(-1)\n",
        "\n",
        "class LightGCNConv(MessagePassing):\n",
        "    def __init__(self): super().__init__(aggr='add')\n",
        "    def forward(self, x, edge_index):\n",
        "        row,col = edge_index\n",
        "        deg = degree(torch.cat([row,col]), x.size(0), dtype=x.dtype)\n",
        "        d   = deg.pow(-0.5); d[d==float('inf')] = 0\n",
        "        norm= d[row]*d[col]\n",
        "        return self.propagate(edge_index, x=x, norm=norm)\n",
        "    def message(self,x_j,norm):\n",
        "        return norm.view(-1,1)*x_j\n",
        "\n",
        "class LightGCN(nn.Module):\n",
        "    def __init__(self, num_users, num_items,\n",
        "                 embedding_dim=64, n_layers=3):\n",
        "        super().__init__()\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.user_emb  = nn.Embedding(num_users, embedding_dim)\n",
        "        self.item_emb  = nn.Embedding(num_items, embedding_dim)\n",
        "        self.conv      = LightGCNConv()\n",
        "        self.n_layers  = n_layers\n",
        "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
        "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
        "\n",
        "    def forward(self, edge_index):\n",
        "        x0 = torch.cat([self.user_emb.weight, self.item_emb.weight], dim=0)\n",
        "        embs = [x0]; x=x0\n",
        "        for _ in range(self.n_layers):\n",
        "            x = self.conv(x, edge_index)\n",
        "            embs.append(x)\n",
        "        return torch.stack(embs,0).mean(0)\n",
        "\n",
        "    def predict(self, edge_index, u, i):\n",
        "        embs = self(edge_index)\n",
        "        return (embs[u]*embs[i]).sum(-1)\n",
        "\n",
        "#===============================================================================\n",
        "# 4) SASRec + train/eval\n",
        "#===============================================================================\n",
        "class SASRecBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.ln1  = nn.LayerNorm(d_model, eps=1e-8)\n",
        "        self.attn = nn.MultiheadAttention(d_model,n_heads,\n",
        "                                          dropout=dropout,\n",
        "                                          batch_first=True)\n",
        "        self.drop1= nn.Dropout(dropout)\n",
        "        self.ln2  = nn.LayerNorm(d_model, eps=1e-8)\n",
        "        self.ffn  = nn.Sequential(\n",
        "            nn.Linear(d_model,d_ff), nn.ReLU(), nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff,d_model), nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x, causal_mask, pad_mask):\n",
        "        z = x; x_n = self.ln1(z)\n",
        "        a,_ = self.attn(x_n, x_n, x_n,\n",
        "                       attn_mask=causal_mask,\n",
        "                       key_padding_mask=pad_mask)\n",
        "        x = z + self.drop1(a)\n",
        "        z2 = x; x2_n = self.ln2(z2)\n",
        "        f  = self.ffn(x2_n)\n",
        "        return z2 + f\n",
        "\n",
        "class SASRec(nn.Module):\n",
        "    PAD = 0\n",
        "    def __init__(self, num_users, num_items,\n",
        "                 maxlen=50, hidden=50,\n",
        "                 blocks=2, heads=1, drop=0.2):\n",
        "        super().__init__()\n",
        "        self.maxlen   = maxlen\n",
        "        self.item_emb = nn.Embedding(num_items+1, hidden, padding_idx=self.PAD)\n",
        "        self.pos_emb  = nn.Embedding(maxlen+1, hidden, padding_idx=0)\n",
        "        self.drop     = nn.Dropout(drop)\n",
        "        self.blocks   = nn.ModuleList([\n",
        "            SASRecBlock(hidden, heads, hidden, drop) for _ in range(blocks)\n",
        "        ])\n",
        "        self.ln_final = nn.LayerNorm(hidden, eps=1e-8)\n",
        "        # init\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Embedding, nn.Linear)):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if hasattr(m,'bias') and m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def log2feats(self, logs):\n",
        "        B,L = logs.shape\n",
        "        x   = self.item_emb(logs) * math.sqrt(self.item_emb.embedding_dim)\n",
        "        pos = torch.arange(1, L+1, device=logs.device).unsqueeze(0).expand(B,-1)\n",
        "        pos = pos * (logs != self.PAD)\n",
        "        x   = x + self.pos_emb(pos)\n",
        "        x   = self.drop(x)\n",
        "        pad_mask = logs.eq(self.PAD)\n",
        "        causal   = torch.triu(\n",
        "            torch.ones(L,L,device=logs.device,dtype=torch.bool), diagonal=1\n",
        "        )\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x, causal_mask=causal, pad_mask=pad_mask)\n",
        "        return self.ln_final(x)\n",
        "\n",
        "    def forward(self, _, log_seqs, pos_seqs, neg_seqs):\n",
        "        device = next(self.parameters()).device\n",
        "        logs = torch.tensor(log_seqs, dtype=torch.long, device=device)\n",
        "        pos  = torch.tensor(pos_seqs, dtype=torch.long, device=device)\n",
        "        neg  = torch.tensor(neg_seqs, dtype=torch.long, device=device)\n",
        "        feats= self.log2feats(logs)        # [B,L,H]\n",
        "        final= feats[:,-1,:]               # [B,H]\n",
        "        pe   = self.item_emb(pos)          # [B,H]\n",
        "        ne   = self.item_emb(neg)          # [B,H]\n",
        "        return (final*pe).sum(-1), (final*ne).sum(-1)\n",
        "\n",
        "    def predict(self, _, log_seqs, item_idx):\n",
        "        device = next(self.parameters()).device\n",
        "        logs = torch.tensor(log_seqs, dtype=torch.long, device=device)\n",
        "        items= torch.tensor(item_idx, dtype=torch.long, device=device)\n",
        "        feats= self.log2feats(logs)        # [B,L,H]\n",
        "        final= feats[:,-1,:]               # [B,H]\n",
        "        ie   = self.item_emb(items)        # [I,H]\n",
        "        return (ie * final).sum(-1)\n",
        "\n",
        "class SASRecDataset(Dataset):\n",
        "    def __init__(self, logs, pos, neg):\n",
        "        self.logs = logs\n",
        "        self.pos  = pos\n",
        "        self.neg  = neg\n",
        "    def __len__(self):\n",
        "        return len(self.logs)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.logs[idx], self.pos[idx], self.neg[idx]\n",
        "\n",
        "def create_sasrec_sequences(df, num_items):\n",
        "    \"\"\"\n",
        "    user -> [shifted+1 item IDs clipped to 1..num_items]\n",
        "    \"\"\"\n",
        "    user_seq = {}\n",
        "    for u, grp in df.sort_values(['user_idx','timestamp']).groupby('user_idx'):\n",
        "        # The clip method here is a safe way to handle potential index errors\n",
        "        seq = (grp.movie_idx.values + 1).clip(1, num_items).astype(int).tolist()\n",
        "        user_seq[u] = seq\n",
        "    return user_seq\n",
        "\n",
        "def create_sasrec_training_data(user_seqs, num_items, maxlen):\n",
        "    \"\"\"\n",
        "    Precompute (log_seq, pos, neg) for each subsequence.\n",
        "    \"\"\"\n",
        "    logs, pos, neg = [], [], []\n",
        "    all_items = set(range(1, num_items+1))\n",
        "    for u, seq in user_seqs.items():\n",
        "        for i in range(1, len(seq)):\n",
        "            inp = seq[:i]\n",
        "            tgt = seq[i]\n",
        "            negs = list(all_items - set(inp))\n",
        "            nid  = np.random.choice(negs)\n",
        "            padded = [0]*(maxlen - len(inp)) + inp[-maxlen:]\n",
        "            logs.append(padded); pos.append(tgt); neg.append(nid)\n",
        "    return (np.array(logs, dtype=np.int64),\n",
        "            np.array(pos,  dtype=np.int64),\n",
        "            np.array(neg,  dtype=np.int64))\n",
        "\n",
        "def train_sasrec(model, dataset, optimizer, device, bs=128):\n",
        "    \"\"\"\n",
        "    One epoch over the SASRecDataset.\n",
        "    \"\"\"\n",
        "    loader = DataLoader(dataset, batch_size=bs, shuffle=True)\n",
        "    model.train()\n",
        "    for logs, pos, neg in loader:\n",
        "        optimizer.zero_grad()\n",
        "        pl, nl = model(None,\n",
        "                       logs.cpu().numpy(),\n",
        "                       pos.cpu().numpy(),\n",
        "                       neg.cpu().numpy())\n",
        "        loss = F.binary_cross_entropy_with_logits(pl, torch.ones_like(pl)) \\\n",
        "             + F.binary_cross_entropy_with_logits(nl, torch.zeros_like(nl))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "def evaluate_sasrec_model(model,\n",
        "                          train_seqs,\n",
        "                          eval_seqs,\n",
        "                          num_items,\n",
        "                          device,\n",
        "                          k=10,\n",
        "                          maxlen=50,\n",
        "                          val_seqs=None):\n",
        "    \"\"\"\n",
        "    Vectorized eval for SASRec: HR/ndcg/prec/rec/f1/mrr\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    users = sorted(set(train_seqs) & set(eval_seqs))\n",
        "    U = len(users)\n",
        "    seqs = torch.zeros((U, maxlen), dtype=torch.long, device=device)\n",
        "\n",
        "    # build prefix matrix\n",
        "    for i,u in enumerate(users):\n",
        "        hist = train_seqs[u].copy()\n",
        "        if val_seqs and u in val_seqs:\n",
        "            hist += val_seqs[u]\n",
        "        tail = hist[-maxlen:]\n",
        "        tail = [min(max(1,int(x)), num_items) for x in tail]\n",
        "        L = len(tail)\n",
        "        if L>0:\n",
        "            seqs[i, maxlen-L:] = torch.tensor(tail, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        feats = model.log2feats(seqs)   # [U,L,H]\n",
        "        final= feats[:,-1,:]            # [U,H]\n",
        "        items= torch.arange(1, num_items+1, device=device)\n",
        "        ie   = model.item_emb(items)    # [I,H]\n",
        "        scores = final @ ie.T           # [U,I]\n",
        "\n",
        "    # mask seen\n",
        "    for i,u in enumerate(users):\n",
        "        seen = set(train_seqs[u])\n",
        "        if val_seqs and u in val_seqs:\n",
        "            seen |= set(val_seqs[u])\n",
        "        mask = [s-1 for s in seen if 1<=s<=num_items]\n",
        "        if mask:\n",
        "            scores[i, mask] = -1e9\n",
        "\n",
        "    topk = torch.topk(scores, k, dim=1).indices.cpu().tolist()\n",
        "    hr, ndcg, prec, rec, f1, mrr = [],[],[],[],[],[]\n",
        "    for i,u in enumerate(users):\n",
        "        truth = set(eval_seqs[u])\n",
        "        hits  = [1 if (t+1) in truth else 0 for t in topk[i]]\n",
        "        hr.append(1.0 if any(hits) else 0.0)\n",
        "        dcg  = sum(h/math.log2(j+2) for j,h in enumerate(hits))\n",
        "        idcg = sum(1.0/math.log2(j+2) for j in range(min(len(truth),k)))\n",
        "        ndcg.append(dcg/idcg if idcg>0 else 0.0)\n",
        "        p = sum(hits)/k\n",
        "        r = sum(hits)/len(truth)\n",
        "        prec.append(p); rec.append(r)\n",
        "        f1.append(2*p*r/(p+r) if (p+r)>0 else 0.0)\n",
        "        mrr.append(next((1.0/(j+1) for j,h in enumerate(hits) if h), 0.0))\n",
        "\n",
        "    return {\n",
        "      'hr':        np.mean(hr),\n",
        "      'ndcg':      np.mean(ndcg),\n",
        "      'precision': np.mean(prec),\n",
        "      'recall':    np.mean(rec),\n",
        "      'f1':        np.mean(f1),\n",
        "      'mrr':       np.mean(mrr)\n",
        "    }\n",
        "\n",
        "#===============================================================================\n",
        "# 5) Cross‐Validation\n",
        "#===============================================================================\n",
        "def robust_cross_validation_graph(model_cls, params, ratings_df,\n",
        "                                  n_folds=3, device='cuda'):\n",
        "    print(f\"\\n=== Graph CV ({n_folds} folds) ===\")\n",
        "    scores=[]\n",
        "    for fold in range(n_folds):\n",
        "        print(f\"\\n-- fold {fold+1}\")\n",
        "        set_seed(42+fold)\n",
        "        tr,va,te,nu,ni = create_train_val_test_split(\n",
        "            ratings_df, min_interactions=3, seed=42+fold\n",
        "        )\n",
        "        train_data = create_pyg_data(tr, nu, ni).to(device)\n",
        "        val_data   = create_pyg_data(va, nu, ni).to(device)\n",
        "        test_data  = create_pyg_data(te, nu, ni).to(device)\n",
        "\n",
        "        model = model_cls(nu, ni, **params).to(device)\n",
        "        opt   = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "        best_hr, pat = 0.0, 0\n",
        "        ckpt = f'gat_fold_{fold}.pt'\n",
        "        for ep in range(50):\n",
        "            train_graph_epoch_regularized(model, train_data, opt, device)\n",
        "            if (ep+1)%5==0:\n",
        "                m = evaluate_graph_model(model, train_data, val_data, device)\n",
        "                print(f\"  ep {ep+1:02d} val HR@10 = {m['hr']:.4f}\")\n",
        "                if m['hr']>best_hr:\n",
        "                    best_hr, pat = m['hr'], 0\n",
        "                    torch.save(model.state_dict(), ckpt)\n",
        "                else:\n",
        "                    pat += 1\n",
        "                if pat>=5:\n",
        "                    print(\"  early stopping\"); break\n",
        "\n",
        "        model.load_state_dict(torch.load(ckpt))\n",
        "        tm = evaluate_graph_model(model, train_data, test_data, device)\n",
        "        print(f\"  TEST HR@10 = {tm['hr']:.4f}\")\n",
        "        scores.append(tm['hr'])\n",
        "        os.remove(ckpt)\n",
        "\n",
        "    m, s = np.mean(scores), np.std(scores)\n",
        "    print(f\"\\nGraph CV HR@10 = {m:.4f} ± {s:.4f}\")\n",
        "    return m, s\n",
        "\n",
        "def robust_cross_validation_sasrec(params, ratings_df,\n",
        "                                   n_folds=3, device='cuda'):\n",
        "    print(f\"\\n=== SASRec CV ({n_folds} folds) ===\")\n",
        "    scores=[]\n",
        "    for fold in range(n_folds):\n",
        "        print(f\"\\n-- fold {fold+1}\")\n",
        "        set_seed(42+fold)\n",
        "        tr,va,te,nu,ni = create_train_val_test_split(\n",
        "            ratings_df, min_interactions=3, seed=42+fold\n",
        "        )\n",
        "        train_seqs = create_sasrec_sequences(tr, num_items=ni)\n",
        "        val_seqs   = create_sasrec_sequences(va, num_items=ni)\n",
        "        test_seqs  = create_sasrec_sequences(te, num_items=ni)\n",
        "\n",
        "        logs,pos,neg = create_sasrec_training_data(train_seqs, ni, params['maxlen'])\n",
        "        ds = SASRecDataset(logs,pos,neg)\n",
        "\n",
        "        model = SASRec(nu, ni,\n",
        "                       maxlen=params['maxlen'],\n",
        "                       hidden=params['hidden'],\n",
        "                       blocks=params['blocks'],\n",
        "                       heads=params['heads'],\n",
        "                       drop=params['drop']).to(device)\n",
        "        opt   = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "        best_hr, pat = 0.0, 0\n",
        "        ckpt = f'sasrec_fold_{fold}.pt'\n",
        "        for ep in range(50):\n",
        "            train_sasrec(model, ds, opt, device)\n",
        "            if (ep+1)%5==0:\n",
        "                m = evaluate_sasrec_model(model,\n",
        "                                          train_seqs, val_seqs,\n",
        "                                          ni, device,\n",
        "                                          k=10, maxlen=params['maxlen'])\n",
        "                print(f\"  ep {ep+1:02d} val HR@10 = {m['hr']:.4f}\")\n",
        "                if m['hr']>best_hr:\n",
        "                    best_hr, pat = m['hr'], 0\n",
        "                    torch.save(model.state_dict(), ckpt)\n",
        "                else:\n",
        "                    pat += 1\n",
        "                if pat>=5:\n",
        "                    print(\"  early stopping\"); break\n",
        "\n",
        "        model.load_state_dict(torch.load(ckpt))\n",
        "        tm = evaluate_sasrec_model(model,\n",
        "                                   train_seqs, test_seqs,\n",
        "                                   ni, device,\n",
        "                                   k=10, maxlen=params['maxlen'],\n",
        "                                   val_seqs=val_seqs)\n",
        "        print(f\"  TEST HR@10 = {tm['hr']:.4f}\")\n",
        "        scores.append(tm['hr'])\n",
        "        os.remove(ckpt)\n",
        "\n",
        "    m,s = np.mean(scores), np.std(scores)\n",
        "    print(f\"\\nSASRec CV HR@10 = {m:.4f} ± {s:.4f}\")\n",
        "    return m, s\n",
        "\n",
        "#===============================================================================\n",
        "# 6) Main\n",
        "#===============================================================================\n",
        "if __name__==\"__main__\":\n",
        "    set_seed(42)\n",
        "    # load ratings.csv\n",
        "    possible = ['/content/drive/MyDrive/movielens/ratings.csv','ratings.csv','data/ratings.csv','./ratings.csv']\n",
        "    path = next((p for p in possible if os.path.exists(p)), None)\n",
        "    if path is None:\n",
        "        raise FileNotFoundError(\"ratings.csv not found\")\n",
        "    ratings_df = pd.read_csv(path)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    # Graph CV\n",
        "    gat_params  = {'embedding_dim':128,'hidden_dim':128,'heads':4,'n_layers':2}\n",
        "    lgcn_params = {'embedding_dim':128,'n_layers':2}\n",
        "\n",
        "    gm,gs = robust_cross_validation_graph(GATRec,   gat_params,  ratings_df,\n",
        "                                         n_folds=3, device=device)\n",
        "    lm,ls = robust_cross_validation_graph(LightGCN, lgcn_params, ratings_df,\n",
        "                                         n_folds=3, device=device)\n",
        "\n",
        "    # SASRec CV\n",
        "    sasrec_params = {'maxlen':50,'hidden':50,'blocks':2,'heads':1,'drop':0.2}\n",
        "    sm,ss = robust_cross_validation_sasrec(sasrec_params, ratings_df,\n",
        "                                           n_folds=3, device=device)\n",
        "\n",
        "    print(\"\\n=== SUMMARY ===\")\n",
        "    print(f\"GATRec   HR@10 = {gm:.4f} ± {gs:.4f}\")\n",
        "    print(f\"LightGCN HR@10 = {lm:.4f} ± {ls:.4f}\")\n",
        "    print(f\"SASRec   HR@10 = {sm:.4f} ± {ss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nVjFrs1eqSU",
        "outputId": "6a2cc21f-cedb-4425-e476-d328906137fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "\n",
            "=== Graph CV (3 folds) ===\n",
            "\n",
            "-- fold 1\n",
            "Split sizes: train=81200, val=9818, test=9818\n",
            "#users=610, #items=8255\n",
            "  ep 05 val HR@10 = 0.1098\n",
            "  ep 10 val HR@10 = 0.1738\n",
            "  ep 15 val HR@10 = 0.2262\n",
            "  ep 20 val HR@10 = 0.2246\n",
            "  ep 25 val HR@10 = 0.2393\n",
            "  ep 30 val HR@10 = 0.2508\n",
            "  ep 35 val HR@10 = 0.2410\n",
            "  ep 40 val HR@10 = 0.2508\n",
            "  ep 45 val HR@10 = 0.2557\n",
            "  ep 50 val HR@10 = 0.2656\n",
            "  TEST HR@10 = 0.2197\n",
            "\n",
            "-- fold 2\n",
            "Split sizes: train=81200, val=9818, test=9818\n",
            "#users=610, #items=8255\n",
            "  ep 05 val HR@10 = 0.1098\n",
            "  ep 10 val HR@10 = 0.1787\n",
            "  ep 15 val HR@10 = 0.2197\n",
            "  ep 20 val HR@10 = 0.2361\n",
            "  ep 25 val HR@10 = 0.2557\n",
            "  ep 30 val HR@10 = 0.2738\n",
            "  ep 35 val HR@10 = 0.2672\n",
            "  ep 40 val HR@10 = 0.2574\n",
            "  ep 45 val HR@10 = 0.2508\n",
            "  ep 50 val HR@10 = 0.2541\n",
            "  TEST HR@10 = 0.2033\n",
            "\n",
            "-- fold 3\n",
            "Split sizes: train=81200, val=9818, test=9818\n",
            "#users=610, #items=8255\n",
            "  ep 05 val HR@10 = 0.1098\n",
            "  ep 10 val HR@10 = 0.1885\n",
            "  ep 15 val HR@10 = 0.2279\n",
            "  ep 20 val HR@10 = 0.2361\n",
            "  ep 25 val HR@10 = 0.2426\n",
            "  ep 30 val HR@10 = 0.2525\n",
            "  ep 35 val HR@10 = 0.2574\n",
            "  ep 40 val HR@10 = 0.2754\n",
            "  ep 45 val HR@10 = 0.2787\n",
            "  ep 50 val HR@10 = 0.2623\n",
            "  TEST HR@10 = 0.2279\n",
            "\n",
            "Graph CV HR@10 = 0.2169 ± 0.0102\n",
            "\n",
            "=== Graph CV (3 folds) ===\n",
            "\n",
            "-- fold 1\n",
            "Split sizes: train=81200, val=9818, test=9818\n",
            "#users=610, #items=8255\n",
            "  ep 05 val HR@10 = 0.2574\n",
            "  ep 10 val HR@10 = 0.2557\n",
            "  ep 15 val HR@10 = 0.2492\n",
            "  ep 20 val HR@10 = 0.2475\n",
            "  ep 25 val HR@10 = 0.2459\n",
            "  ep 30 val HR@10 = 0.2492\n",
            "  early stopping\n",
            "  TEST HR@10 = 0.1902\n",
            "\n",
            "-- fold 2\n",
            "Split sizes: train=81200, val=9818, test=9818\n",
            "#users=610, #items=8255\n",
            "  ep 05 val HR@10 = 0.2525\n",
            "  ep 10 val HR@10 = 0.2574\n",
            "  ep 15 val HR@10 = 0.2492\n",
            "  ep 20 val HR@10 = 0.2525\n",
            "  ep 25 val HR@10 = 0.2492\n",
            "  ep 30 val HR@10 = 0.2508\n",
            "  ep 35 val HR@10 = 0.2525\n",
            "  early stopping\n",
            "  TEST HR@10 = 0.2082\n",
            "\n",
            "-- fold 3\n",
            "Split sizes: train=81200, val=9818, test=9818\n",
            "#users=610, #items=8255\n",
            "  ep 05 val HR@10 = 0.2443\n",
            "  ep 10 val HR@10 = 0.2508\n",
            "  ep 15 val HR@10 = 0.2508\n",
            "  ep 20 val HR@10 = 0.2508\n",
            "  ep 25 val HR@10 = 0.2607\n",
            "  ep 30 val HR@10 = 0.2525\n",
            "  ep 35 val HR@10 = 0.2492\n",
            "  ep 40 val HR@10 = 0.2459\n",
            "  ep 45 val HR@10 = 0.2475\n",
            "  ep 50 val HR@10 = 0.2443\n",
            "  early stopping\n",
            "  TEST HR@10 = 0.2082\n",
            "\n",
            "Graph CV HR@10 = 0.2022 ± 0.0085\n",
            "\n",
            "=== SASRec CV (3 folds) ===\n",
            "\n",
            "-- fold 1\n",
            "Split sizes: train=81200, val=9818, test=9818\n",
            "#users=610, #items=8255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-8f699e4fe0b4>:367: RuntimeWarning: invalid value encountered in cast\n",
            "  seq = (grp.movie_idx.values + 1).clip(1, num_items).astype(int).tolist()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ep 05 val HR@10 = 0.0820\n",
            "  ep 10 val HR@10 = 0.0820\n",
            "  ep 15 val HR@10 = 0.0820\n",
            "  ep 20 val HR@10 = 0.0820\n",
            "  ep 25 val HR@10 = 0.0820\n",
            "  ep 30 val HR@10 = 0.0820\n",
            "  early stopping\n",
            "  TEST HR@10 = 0.0918\n",
            "\n",
            "-- fold 2\n",
            "Split sizes: train=81200, val=9818, test=9818\n",
            "#users=610, #items=8255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-8f699e4fe0b4>:367: RuntimeWarning: invalid value encountered in cast\n",
            "  seq = (grp.movie_idx.values + 1).clip(1, num_items).astype(int).tolist()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ep 05 val HR@10 = 0.0820\n",
            "  ep 10 val HR@10 = 0.0820\n",
            "  ep 15 val HR@10 = 0.0820\n",
            "  ep 20 val HR@10 = 0.0820\n",
            "  ep 25 val HR@10 = 0.0820\n",
            "  ep 30 val HR@10 = 0.0820\n",
            "  early stopping\n",
            "  TEST HR@10 = 0.0918\n",
            "\n",
            "-- fold 3\n",
            "Split sizes: train=81200, val=9818, test=9818\n",
            "#users=610, #items=8255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-8f699e4fe0b4>:367: RuntimeWarning: invalid value encountered in cast\n",
            "  seq = (grp.movie_idx.values + 1).clip(1, num_items).astype(int).tolist()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ep 05 val HR@10 = 0.0820\n",
            "  ep 10 val HR@10 = 0.0820\n",
            "  ep 15 val HR@10 = 0.0820\n",
            "  ep 20 val HR@10 = 0.0820\n",
            "  ep 25 val HR@10 = 0.0820\n",
            "  ep 30 val HR@10 = 0.0820\n",
            "  early stopping\n",
            "  TEST HR@10 = 0.0918\n",
            "\n",
            "SASRec CV HR@10 = 0.0918 ± 0.0000\n",
            "\n",
            "=== SUMMARY ===\n",
            "GATRec   HR@10 = 0.2169 ± 0.0102\n",
            "LightGCN HR@10 = 0.2022 ± 0.0085\n",
            "SASRec   HR@10 = 0.0918 ± 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GATConv, MessagePassing\n",
        "from torch_geometric.utils import degree\n",
        "from collections import defaultdict\n",
        "\n",
        "#===============================================================================\n",
        "# 1) Utilities\n",
        "#===============================================================================\n",
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark    = False\n",
        "\n",
        "def create_train_val_test_split(ratings_df,\n",
        "                                val_ratio=0.1,\n",
        "                                test_ratio=0.1,\n",
        "                                min_interactions=1,\n",
        "                                positive_threshold=None,\n",
        "                                seed=42):\n",
        "    \"\"\"\n",
        "    Chronological per-user train/val/test split.\n",
        "    Returns train_df, val_df, test_df, num_users, num_items\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    df = ratings_df.copy()\n",
        "    if positive_threshold is not None:\n",
        "        df = df[df.rating >= positive_threshold]\n",
        "\n",
        "    counts = df.user_id.value_counts()  # Changed from userId to user_id\n",
        "    valid  = counts[counts >= min_interactions].index\n",
        "    df = df[df.user_id.isin(valid)]     # Changed from userId to user_id\n",
        "    df = df.sort_values(['user_id','timestamp'])  # Changed from userId to user_id\n",
        "\n",
        "    trains, vals, tests = [], [], []\n",
        "    for _, udf in df.groupby('user_id'):  # Changed from userId to user_id\n",
        "        n = len(udf)\n",
        "        n_test  = max(1, int(n*test_ratio))\n",
        "        n_val   = max(1, int(n*val_ratio))\n",
        "        n_train = n - n_val - n_test\n",
        "        if n_train < 1:\n",
        "            continue\n",
        "        trains.append( udf.iloc[:n_train] )\n",
        "        vals.append(   udf.iloc[n_train:n_train+n_val] )\n",
        "        tests.append(  udf.iloc[n_train+n_val:] )\n",
        "\n",
        "    train_df = pd.concat(trains, ignore_index=True)\n",
        "    val_df   = pd.concat(vals,   ignore_index=True)\n",
        "    test_df  = pd.concat(tests,  ignore_index=True)\n",
        "\n",
        "    # remap to contiguous 0..N-1\n",
        "    u2idx = {u:i for i,u in enumerate(train_df.user_id.unique())}    # Changed from userId\n",
        "    i2idx = {m:i for i,m in enumerate(train_df.item_id.unique())}    # Changed from movieId to item_id\n",
        "\n",
        "    for d in (train_df, val_df, test_df):\n",
        "        d['user_idx']  = d.user_id .map(u2idx)   # Changed from userId\n",
        "        d['movie_idx'] = d.item_id.map(i2idx)    # Changed from movieId to item_id\n",
        "\n",
        "    num_users = len(u2idx)\n",
        "    num_items = len(i2idx)\n",
        "    print(f\"Split sizes: train={len(train_df)}, val={len(val_df)}, test={len(test_df)}\")\n",
        "    print(f\"#users={num_users}, #items={num_items}\")\n",
        "    return train_df, val_df, test_df, num_users, num_items\n",
        "\n",
        "def create_pyg_data(df, num_users, num_items):\n",
        "    \"\"\"\n",
        "    Build a PyG bipartite graph.\n",
        "    Users: 0..num_users-1, items: num_users..num_users+num_items-1\n",
        "    \"\"\"\n",
        "    us  = torch.LongTensor(df.user_idx.values)\n",
        "    is_ = torch.LongTensor(df.movie_idx.values) + num_users\n",
        "    edge_index = torch.stack([torch.cat([us,is_]), torch.cat([is_,us])], dim=0)\n",
        "    data = Data(edge_index=edge_index, num_nodes=num_users+num_items)\n",
        "    data.num_users = num_users\n",
        "    data.num_items = num_items\n",
        "    data.orig_interactions = torch.stack([us,is_], dim=1)\n",
        "    return data\n",
        "\n",
        "#===============================================================================\n",
        "# 2) Graph eval + train\n",
        "#===============================================================================\n",
        "def evaluate_graph_model(model, train_data, eval_data, device, k=10):\n",
        "    \"\"\"\n",
        "    HR@k, NDCG@k, precision, recall, f1, mrr for graph-based models\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    metrics = defaultdict(list)\n",
        "\n",
        "    # 1) compute embeddings on the correct device\n",
        "    with torch.no_grad():\n",
        "        edge_index = train_data.edge_index.to(device)\n",
        "        embs = model(edge_index)\n",
        "        U = model.num_users\n",
        "        user_embs = embs[:U]\n",
        "        item_embs = embs[U:]\n",
        "\n",
        "    # 2) build train‐map and eval‐map on CPU\n",
        "    train_map = defaultdict(set)\n",
        "    for u,i in train_data.orig_interactions.cpu().tolist():\n",
        "        train_map[u].add(i - U)\n",
        "\n",
        "    eval_map = defaultdict(list)\n",
        "    for u,i in eval_data.orig_interactions.cpu().tolist():\n",
        "        eval_map[u].append(i - U)\n",
        "\n",
        "    # 3) per‐user ranking\n",
        "    for u, true_items in eval_map.items():\n",
        "        if not true_items:\n",
        "            continue\n",
        "\n",
        "        # Perform multiplication on the GPU, then move result to CPU\n",
        "        scores = (item_embs @ user_embs[u]).cpu().numpy()\n",
        "\n",
        "        # mask training items\n",
        "        for ti in train_map[u]:\n",
        "            scores[ti] = -1e9\n",
        "\n",
        "        # top‐k\n",
        "        topk = np.argpartition(-scores, k)[:k]\n",
        "        hits = [1 if t in true_items else 0 for t in topk]\n",
        "\n",
        "        # HR\n",
        "        hr = 1.0 if any(hits) else 0.0\n",
        "        # NDCG\n",
        "        dcg  = sum(h / math.log2(idx+2) for idx,h in enumerate(hits))\n",
        "        idcg = sum(1.0/math.log2(i+2) for i in range(min(len(true_items), k)))\n",
        "        ndcg = dcg/idcg if idcg>0 else 0.0\n",
        "        # prec/rec/f1\n",
        "        prec = sum(hits)/k\n",
        "        rec  = sum(hits)/len(true_items)\n",
        "        f1   = 2*prec*rec/(prec+rec) if (prec+rec)>0 else 0.0\n",
        "        # MRR\n",
        "        mrr  = next((1.0/(idx+1) for idx,h in enumerate(hits) if h), 0.0)\n",
        "\n",
        "        metrics['hr'].append(hr)\n",
        "        metrics['ndcg'].append(ndcg)\n",
        "        metrics['precision'].append(prec)\n",
        "        metrics['recall'].append(rec)\n",
        "        metrics['f1'].append(f1)\n",
        "        metrics['mrr'].append(mrr)\n",
        "\n",
        "    return {m: np.mean(v) for m,v in metrics.items()}\n",
        "\n",
        "def train_graph_epoch_regularized(model,\n",
        "                                  train_data,\n",
        "                                  optimizer,\n",
        "                                  device,\n",
        "                                  batch_size=16384,\n",
        "                                  l2_reg=1e-4):\n",
        "    \"\"\"\n",
        "    One epoch of BPR + L2 training for graph models.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    edge_index = train_data.edge_index.to(device)\n",
        "    pos_edges  = train_data.orig_interactions.to(device)\n",
        "    E = pos_edges.size(0)\n",
        "    perm = torch.randperm(E, device=device)\n",
        "    n_batches = (E + batch_size - 1) // batch_size\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for b in range(n_batches):\n",
        "        idx   = perm[b*batch_size:(b+1)*batch_size]\n",
        "        users = pos_edges[idx,0]\n",
        "        items = pos_edges[idx,1]\n",
        "        negs  = torch.randint(model.num_users,\n",
        "                              model.num_users+model.num_items,\n",
        "                              size=users.size(), device=device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pos_scores = model.predict(edge_index, users, items)\n",
        "        neg_scores = model.predict(edge_index, users, negs)\n",
        "        bpr_loss = -torch.log(torch.sigmoid(pos_scores - neg_scores)+1e-8).mean()\n",
        "        l2_loss  = l2_reg * (\n",
        "                model.user_emb(users).pow(2).mean()\n",
        "              + model.item_emb(items-model.num_users).pow(2).mean()\n",
        "              + model.item_emb(negs-model.num_users).pow(2).mean()\n",
        "        )\n",
        "        loss = bpr_loss + l2_loss\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / n_batches\n",
        "\n",
        "#===============================================================================\n",
        "# 3) Graph Models\n",
        "#===============================================================================\n",
        "class GATRec(nn.Module):\n",
        "    def __init__(self, num_users, num_items,\n",
        "                 embedding_dim=64, hidden_dim=64,\n",
        "                 heads=4, n_layers=3):\n",
        "        super().__init__()\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.user_emb  = nn.Embedding(num_users, embedding_dim)\n",
        "        self.item_emb  = nn.Embedding(num_items, embedding_dim)\n",
        "        self.convs     = nn.ModuleList()\n",
        "        # first layer\n",
        "        self.convs.append(GATConv(embedding_dim, hidden_dim, heads=heads, dropout=0.6))\n",
        "        # middle\n",
        "        for _ in range(n_layers-2):\n",
        "            self.convs.append(GATConv(hidden_dim*heads, hidden_dim, heads=heads, dropout=0.6))\n",
        "        # final\n",
        "        self.convs.append(GATConv(hidden_dim*heads, hidden_dim, heads=1, concat=False, dropout=0.6))\n",
        "        self.dropout = nn.Dropout(0.6)\n",
        "        # init\n",
        "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
        "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
        "\n",
        "    def forward(self, edge_index):\n",
        "        x = torch.cat([self.user_emb.weight, self.item_emb.weight], dim=0)\n",
        "        for conv in self.convs[:-1]:\n",
        "            x = conv(x, edge_index)\n",
        "            x = F.relu(x); x = self.dropout(x)\n",
        "        x = self.convs[-1](x, edge_index)\n",
        "        return x\n",
        "\n",
        "    def predict(self, edge_index, u, i):\n",
        "        embs = self(edge_index)\n",
        "        return (embs[u]*embs[i]).sum(-1)\n",
        "\n",
        "class LightGCNConv(MessagePassing):\n",
        "    def __init__(self): super().__init__(aggr='add')\n",
        "    def forward(self, x, edge_index):\n",
        "        row,col = edge_index\n",
        "        deg = degree(torch.cat([row,col]), x.size(0), dtype=x.dtype)\n",
        "        d   = deg.pow(-0.5); d[d==float('inf')] = 0\n",
        "        norm= d[row]*d[col]\n",
        "        return self.propagate(edge_index, x=x, norm=norm)\n",
        "    def message(self,x_j,norm):\n",
        "        return norm.view(-1,1)*x_j\n",
        "\n",
        "class LightGCN(nn.Module):\n",
        "    def __init__(self, num_users, num_items,\n",
        "                 embedding_dim=64, n_layers=3):\n",
        "        super().__init__()\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.user_emb  = nn.Embedding(num_users, embedding_dim)\n",
        "        self.item_emb  = nn.Embedding(num_items, embedding_dim)\n",
        "        self.conv      = LightGCNConv()\n",
        "        self.n_layers  = n_layers\n",
        "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
        "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
        "\n",
        "    def forward(self, edge_index):\n",
        "        x0 = torch.cat([self.user_emb.weight, self.item_emb.weight], dim=0)\n",
        "        embs = [x0]; x=x0\n",
        "        for _ in range(self.n_layers):\n",
        "            x = self.conv(x, edge_index)\n",
        "            embs.append(x)\n",
        "        return torch.stack(embs,0).mean(0)\n",
        "\n",
        "    def predict(self, edge_index, u, i):\n",
        "        embs = self(edge_index)\n",
        "        return (embs[u]*embs[i]).sum(-1)\n",
        "\n",
        "#===============================================================================\n",
        "# 4) SASRec + train/eval\n",
        "#===============================================================================\n",
        "class SASRecBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.ln1  = nn.LayerNorm(d_model, eps=1e-8)\n",
        "        self.attn = nn.MultiheadAttention(d_model,n_heads,\n",
        "                                          dropout=dropout,\n",
        "                                          batch_first=True)\n",
        "        self.drop1= nn.Dropout(dropout)\n",
        "        self.ln2  = nn.LayerNorm(d_model, eps=1e-8)\n",
        "        self.ffn  = nn.Sequential(\n",
        "            nn.Linear(d_model,d_ff), nn.ReLU(), nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff,d_model), nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x, causal_mask, pad_mask):\n",
        "        z = x; x_n = self.ln1(z)\n",
        "        a,_ = self.attn(x_n, x_n, x_n,\n",
        "                       attn_mask=causal_mask,\n",
        "                       key_padding_mask=pad_mask)\n",
        "        x = z + self.drop1(a)\n",
        "        z2 = x; x2_n = self.ln2(z2)\n",
        "        f  = self.ffn(x2_n)\n",
        "        return z2 + f\n",
        "\n",
        "class SASRec(nn.Module):\n",
        "    PAD = 0\n",
        "    def __init__(self, num_users, num_items,\n",
        "                 maxlen=50, hidden=50,\n",
        "                 blocks=2, heads=1, drop=0.2):\n",
        "        super().__init__()\n",
        "        self.maxlen   = maxlen\n",
        "        self.item_emb = nn.Embedding(num_items+1, hidden, padding_idx=self.PAD)\n",
        "        self.pos_emb  = nn.Embedding(maxlen+1, hidden, padding_idx=0)\n",
        "        self.drop     = nn.Dropout(drop)\n",
        "        self.blocks   = nn.ModuleList([\n",
        "            SASRecBlock(hidden, heads, hidden, drop) for _ in range(blocks)\n",
        "        ])\n",
        "        self.ln_final = nn.LayerNorm(hidden, eps=1e-8)\n",
        "        # init\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Embedding, nn.Linear)):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if hasattr(m,'bias') and m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def log2feats(self, logs):\n",
        "        B,L = logs.shape\n",
        "        x   = self.item_emb(logs) * math.sqrt(self.item_emb.embedding_dim)\n",
        "        pos = torch.arange(1, L+1, device=logs.device).unsqueeze(0).expand(B,-1)\n",
        "        pos = pos * (logs != self.PAD)\n",
        "        x   = x + self.pos_emb(pos)\n",
        "        x   = self.drop(x)\n",
        "        pad_mask = logs.eq(self.PAD)\n",
        "        causal   = torch.triu(\n",
        "            torch.ones(L,L,device=logs.device,dtype=torch.bool), diagonal=1\n",
        "        )\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x, causal_mask=causal, pad_mask=pad_mask)\n",
        "        return self.ln_final(x)\n",
        "\n",
        "    def forward(self, _, log_seqs, pos_seqs, neg_seqs):\n",
        "        device = next(self.parameters()).device\n",
        "        logs = torch.tensor(log_seqs, dtype=torch.long, device=device)\n",
        "        pos  = torch.tensor(pos_seqs, dtype=torch.long, device=device)\n",
        "        neg  = torch.tensor(neg_seqs, dtype=torch.long, device=device)\n",
        "        feats= self.log2feats(logs)        # [B,L,H]\n",
        "        final= feats[:,-1,:]               # [B,H]\n",
        "        pe   = self.item_emb(pos)          # [B,H]\n",
        "        ne   = self.item_emb(neg)          # [B,H]\n",
        "        return (final*pe).sum(-1), (final*ne).sum(-1)\n",
        "\n",
        "    def predict(self, _, log_seqs, item_idx):\n",
        "        device = next(self.parameters()).device\n",
        "        logs = torch.tensor(log_seqs, dtype=torch.long, device=device)\n",
        "        items= torch.tensor(item_idx, dtype=torch.long, device=device)\n",
        "        feats= self.log2feats(logs)        # [B,L,H]\n",
        "        final= feats[:,-1,:]               # [B,H]\n",
        "        ie   = self.item_emb(items)        # [I,H]\n",
        "        return (ie * final).sum(-1)\n",
        "\n",
        "class SASRecDataset(Dataset):\n",
        "    def __init__(self, logs, pos, neg):\n",
        "        self.logs = logs\n",
        "        self.pos  = pos\n",
        "        self.neg  = neg\n",
        "    def __len__(self):\n",
        "        return len(self.logs)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.logs[idx], self.pos[idx], self.neg[idx]\n",
        "\n",
        "def create_sasrec_sequences(df, num_items):\n",
        "    \"\"\"\n",
        "    user -> [shifted+1 item IDs clipped to 1..num_items]\n",
        "    \"\"\"\n",
        "    user_seq = {}\n",
        "    for u, grp in df.sort_values(['user_idx','timestamp']).groupby('user_idx'):\n",
        "        # The clip method here is a safe way to handle potential index errors\n",
        "        seq = (grp.movie_idx.values + 1).clip(1, num_items).astype(int).tolist()\n",
        "        user_seq[u] = seq\n",
        "    return user_seq\n",
        "\n",
        "def create_sasrec_training_data(user_seqs, num_items, maxlen):\n",
        "    \"\"\"\n",
        "    Precompute (log_seq, pos, neg) for each subsequence.\n",
        "    \"\"\"\n",
        "    logs, pos, neg = [], [], []\n",
        "    all_items = set(range(1, num_items+1))\n",
        "    for u, seq in user_seqs.items():\n",
        "        for i in range(1, len(seq)):\n",
        "            inp = seq[:i]\n",
        "            tgt = seq[i]\n",
        "            negs = list(all_items - set(inp))\n",
        "            nid  = np.random.choice(negs)\n",
        "            padded = [0]*(maxlen - len(inp)) + inp[-maxlen:]\n",
        "            logs.append(padded); pos.append(tgt); neg.append(nid)\n",
        "    return (np.array(logs, dtype=np.int64),\n",
        "            np.array(pos,  dtype=np.int64),\n",
        "            np.array(neg,  dtype=np.int64))\n",
        "\n",
        "def train_sasrec(model, dataset, optimizer, device, bs=512):  # INCREASED BATCH SIZE\n",
        "    \"\"\"\n",
        "    One epoch over the SASRecDataset.\n",
        "    \"\"\"\n",
        "    loader = DataLoader(dataset, batch_size=bs, shuffle=True)\n",
        "    model.train()\n",
        "    for logs, pos, neg in loader:\n",
        "        optimizer.zero_grad()\n",
        "        pl, nl = model(None,\n",
        "                       logs.cpu().numpy(),\n",
        "                       pos.cpu().numpy(),\n",
        "                       neg.cpu().numpy())\n",
        "        loss = F.binary_cross_entropy_with_logits(pl, torch.ones_like(pl)) \\\n",
        "             + F.binary_cross_entropy_with_logits(nl, torch.zeros_like(nl))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "def evaluate_sasrec_model(model,\n",
        "                          train_seqs,\n",
        "                          eval_seqs,\n",
        "                          num_items,\n",
        "                          device,\n",
        "                          k=10,\n",
        "                          maxlen=50,\n",
        "                          val_seqs=None):\n",
        "    \"\"\"\n",
        "    Vectorized eval for SASRec: HR/ndcg/prec/rec/f1/mrr\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    users = sorted(set(train_seqs) & set(eval_seqs))\n",
        "    U = len(users)\n",
        "    seqs = torch.zeros((U, maxlen), dtype=torch.long, device=device)\n",
        "\n",
        "    # build prefix matrix\n",
        "    for i,u in enumerate(users):\n",
        "        hist = train_seqs[u].copy()\n",
        "        if val_seqs and u in val_seqs:\n",
        "            hist += val_seqs[u]\n",
        "        tail = hist[-maxlen:]\n",
        "        tail = [min(max(1,int(x)), num_items) for x in tail]\n",
        "        L = len(tail)\n",
        "        if L>0:\n",
        "            seqs[i, maxlen-L:] = torch.tensor(tail, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        feats = model.log2feats(seqs)   # [U,L,H]\n",
        "        final= feats[:,-1,:]            # [U,H]\n",
        "        items= torch.arange(1, num_items+1, device=device)\n",
        "        ie   = model.item_emb(items)    # [I,H]\n",
        "        scores = final @ ie.T           # [U,I]\n",
        "\n",
        "    # mask seen\n",
        "    for i,u in enumerate(users):\n",
        "        seen = set(train_seqs[u])\n",
        "        if val_seqs and u in val_seqs:\n",
        "            seen |= set(val_seqs[u])\n",
        "        mask = [s-1 for s in seen if 1<=s<=num_items]\n",
        "        if mask:\n",
        "            scores[i, mask] = -1e9\n",
        "\n",
        "    topk = torch.topk(scores, k, dim=1).indices.cpu().tolist()\n",
        "    hr, ndcg, prec, rec, f1, mrr = [],[],[],[],[],[]\n",
        "    for i,u in enumerate(users):\n",
        "        truth = set(eval_seqs[u])\n",
        "        hits  = [1 if (t+1) in truth else 0 for t in topk[i]]\n",
        "        hr.append(1.0 if any(hits) else 0.0)\n",
        "        dcg  = sum(h/math.log2(j+2) for j,h in enumerate(hits))\n",
        "        idcg = sum(1.0/math.log2(j+2) for j in range(min(len(truth),k)))\n",
        "        ndcg.append(dcg/idcg if idcg>0 else 0.0)\n",
        "        p = sum(hits)/k\n",
        "        r = sum(hits)/len(truth)\n",
        "        prec.append(p); rec.append(r)\n",
        "        f1.append(2*p*r/(p+r) if (p+r)>0 else 0.0)\n",
        "        mrr.append(next((1.0/(j+1) for j,h in enumerate(hits) if h), 0.0))\n",
        "\n",
        "    return {\n",
        "      'hr':        np.mean(hr),\n",
        "      'ndcg':      np.mean(ndcg),\n",
        "      'precision': np.mean(prec),\n",
        "      'recall':    np.mean(rec),\n",
        "      'f1':        np.mean(f1),\n",
        "      'mrr':       np.mean(mrr)\n",
        "    }\n",
        "\n",
        "#===============================================================================\n",
        "# 5) Cross‐Validation\n",
        "#===============================================================================\n",
        "def robust_cross_validation_graph(model_cls, params, ratings_df,\n",
        "                                  n_folds=3, device='cuda'):\n",
        "    print(f\"\\n=== Graph CV ({n_folds} folds) ===\")\n",
        "    scores=[]\n",
        "    for fold in range(n_folds):\n",
        "        print(f\"\\n-- fold {fold+1}\")\n",
        "        set_seed(42+fold)\n",
        "        tr,va,te,nu,ni = create_train_val_test_split(\n",
        "            ratings_df, min_interactions=5, seed=42+fold\n",
        "        )\n",
        "        train_data = create_pyg_data(tr, nu, ni).to(device)\n",
        "        val_data   = create_pyg_data(va, nu, ni).to(device)\n",
        "        test_data  = create_pyg_data(te, nu, ni).to(device)\n",
        "\n",
        "        model = model_cls(nu, ni, **params).to(device)\n",
        "        opt   = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "        best_hr, pat = 0.0, 0\n",
        "        ckpt = f'gat_fold_{fold}.pt'\n",
        "        for ep in range(201):  # CHANGED TO 201 EPOCHS\n",
        "            train_graph_epoch_regularized(model, train_data, opt, device)\n",
        "            if (ep+1)%20==0:  # EVALUATE EVERY 20 EPOCHS\n",
        "                m = evaluate_graph_model(model, train_data, val_data, device)\n",
        "                print(f\"  ep {ep+1:02d} val HR@10 = {m['hr']:.4f}\")\n",
        "                if m['hr']>best_hr:\n",
        "                    best_hr, pat = m['hr'], 0\n",
        "                    torch.save(model.state_dict(), ckpt)\n",
        "                else:\n",
        "                    pat += 1\n",
        "                if pat>=5:\n",
        "                    print(\"  early stopping\"); break\n",
        "\n",
        "        model.load_state_dict(torch.load(ckpt))\n",
        "        tm = evaluate_graph_model(model, train_data, test_data, device)\n",
        "        print(f\"  TEST HR@10 = {tm['hr']:.4f}\")\n",
        "        scores.append(tm['hr'])\n",
        "        os.remove(ckpt)\n",
        "\n",
        "    m, s = np.mean(scores), np.std(scores)\n",
        "    print(f\"\\nGraph CV HR@10 = {m:.4f} ± {s:.4f}\")\n",
        "    return m, s\n",
        "\n",
        "def robust_cross_validation_sasrec(params, ratings_df,\n",
        "                                   n_folds=3, device='cuda'):\n",
        "    print(f\"\\n=== SASRec CV ({n_folds} folds) ===\")\n",
        "    scores=[]\n",
        "    for fold in range(n_folds):\n",
        "        print(f\"\\n-- fold {fold+1}\")\n",
        "        set_seed(42+fold)\n",
        "        tr,va,te,nu,ni = create_train_val_test_split(\n",
        "            ratings_df, min_interactions=5, seed=42+fold\n",
        "        )\n",
        "        train_seqs = create_sasrec_sequences(tr, num_items=ni)\n",
        "        val_seqs   = create_sasrec_sequences(va, num_items=ni)\n",
        "        test_seqs  = create_sasrec_sequences(te, num_items=ni)\n",
        "\n",
        "        logs,pos,neg = create_sasrec_training_data(train_seqs, ni, params['maxlen'])\n",
        "        ds = SASRecDataset(logs,pos,neg)\n",
        "\n",
        "        model = SASRec(nu, ni,\n",
        "                       maxlen=params['maxlen'],\n",
        "                       hidden=params['hidden'],\n",
        "                       blocks=params['blocks'],\n",
        "                       heads=params['heads'],\n",
        "                       drop=params['drop']).to(device)\n",
        "        opt   = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "        best_hr, pat = 0.0, 0\n",
        "        ckpt = f'sasrec_fold_{fold}.pt'\n",
        "        for ep in range(201):  # CHANGED TO 201 EPOCHS\n",
        "            train_sasrec(model, ds, opt, device)\n",
        "            if (ep+1)%20==0:  # EVALUATE EVERY 20 EPOCHS\n",
        "                m = evaluate_sasrec_model(model,\n",
        "                                          train_seqs, val_seqs,\n",
        "                                          ni, device,\n",
        "                                          k=10, maxlen=params['maxlen'])\n",
        "                print(f\"  ep {ep+1:02d} val HR@10 = {m['hr']:.4f}\")\n",
        "                if m['hr']>best_hr:\n",
        "                    best_hr, pat = m['hr'], 0\n",
        "                    torch.save(model.state_dict(), ckpt)\n",
        "                else:\n",
        "                    pat += 1\n",
        "                if pat>=5:\n",
        "                    print(\"  early stopping\"); break\n",
        "\n",
        "        model.load_state_dict(torch.load(ckpt))\n",
        "        tm = evaluate_sasrec_model(model,\n",
        "                                   train_seqs, test_seqs,\n",
        "                                   ni, device,\n",
        "                                   k=10, maxlen=params['maxlen'],\n",
        "                                   val_seqs=val_seqs)\n",
        "        print(f\"  TEST HR@10 = {tm['hr']:.4f}\")\n",
        "        scores.append(tm['hr'])\n",
        "        os.remove(ckpt)\n",
        "\n",
        "    m,s = np.mean(scores), np.std(scores)\n",
        "    print(f\"\\nSASRec CV HR@10 = {m:.4f} ± {s:.4f}\")\n",
        "    return m, s\n",
        "\n",
        "#===============================================================================\n",
        "# 6) Main - MODIFIED FOR ML-1M DATASET\n",
        "#===============================================================================\n",
        "if __name__==\"__main__\":\n",
        "    set_seed(42)\n",
        "\n",
        "    # Load ml-1m.txt dataset (space-separated: user_id item_id)\n",
        "    possible = ['/content/data/ml-1m.txt', 'data/ml-1m.txt', './ml-1m.txt', 'ml-1m.txt']\n",
        "    path = next((p for p in possible if os.path.exists(p)), None)\n",
        "    if path is None:\n",
        "        raise FileNotFoundError(\"ml-1m.txt not found\")\n",
        "\n",
        "    # Read the ml-1m.txt file (format: user_id item_id per line)\n",
        "    data = []\n",
        "    with open(path, 'r') as f:\n",
        "        for line_num, line in enumerate(f):\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) >= 2:\n",
        "                try:\n",
        "                    user_id = int(parts[0])\n",
        "                    item_id = int(parts[1])\n",
        "                    data.append([user_id, item_id, line_num])  # Use line number as timestamp\n",
        "                except ValueError:\n",
        "                    continue\n",
        "\n",
        "    # Create DataFrame with the correct column names\n",
        "    ratings_df = pd.DataFrame(data, columns=['user_id', 'item_id', 'timestamp'])\n",
        "    print(f\"Loaded {len(ratings_df)} interactions from ml-1m dataset\")\n",
        "    print(f\"Users: {ratings_df.user_id.nunique()}, Items: {ratings_df.item_id.nunique()}\")\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    # Graph CV\n",
        "    gat_params  = {'embedding_dim':128,'hidden_dim':128,'heads':4,'n_layers':2}\n",
        "    lgcn_params = {'embedding_dim':128,'n_layers':2}\n",
        "\n",
        "    gm,gs = robust_cross_validation_graph(GATRec,   gat_params,  ratings_df,\n",
        "                                         n_folds=3, device=device)\n",
        "    lm,ls = robust_cross_validation_graph(LightGCN, lgcn_params, ratings_df,\n",
        "                                         n_folds=3, device=device)\n",
        "\n",
        "    # SASRec CV\n",
        "    sasrec_params = {'maxlen':50,'hidden':50,'blocks':2,'heads':1,'drop':0.2}\n",
        "    sm,ss = robust_cross_validation_sasrec(sasrec_params, ratings_df,\n",
        "                                           n_folds=3, device=device)\n",
        "\n",
        "    print(\"\\n=== SUMMARY ===\")\n",
        "    print(f\"GATRec   HR@10 = {gm:.4f} ± {gs:.4f}\")\n",
        "    print(f\"LightGCN HR@10 = {lm:.4f} ± {ls:.4f}\")\n",
        "    print(f\"SASRec   HR@10 = {sm:.4f} ± {ss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1ofhgkgPLMS",
        "outputId": "902ba565-43e4-486a-ae5d-85c3840e7067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 999611 interactions from ml-1m dataset\n",
            "Users: 6040, Items: 3416\n",
            "Device: cuda\n",
            "\n",
            "=== Graph CV (3 folds) ===\n",
            "\n",
            "-- fold 1\n",
            "Split sizes: train=804987, val=97312, test=97312\n",
            "#users=6040, #items=3415\n",
            "  ep 20 val HR@10 = 0.2791\n",
            "  ep 40 val HR@10 = 0.2854\n",
            "  ep 60 val HR@10 = 0.3263\n",
            "  ep 80 val HR@10 = 0.3243\n",
            "  ep 100 val HR@10 = 0.3310\n",
            "  ep 120 val HR@10 = 0.3379\n",
            "  ep 140 val HR@10 = 0.3389\n",
            "  ep 160 val HR@10 = 0.3411\n",
            "  ep 180 val HR@10 = 0.3417\n",
            "  ep 200 val HR@10 = 0.3483\n",
            "  TEST HR@10 = 0.3005\n",
            "\n",
            "-- fold 2\n",
            "Split sizes: train=804987, val=97312, test=97312\n",
            "#users=6040, #items=3415\n",
            "  ep 20 val HR@10 = 0.3003\n",
            "  ep 40 val HR@10 = 0.2972\n",
            "  ep 60 val HR@10 = 0.2907\n",
            "  ep 80 val HR@10 = 0.3288\n",
            "  ep 100 val HR@10 = 0.3250\n",
            "  ep 120 val HR@10 = 0.3315\n",
            "  ep 140 val HR@10 = 0.3275\n",
            "  ep 160 val HR@10 = 0.3341\n",
            "  ep 180 val HR@10 = 0.3353\n",
            "  ep 200 val HR@10 = 0.3348\n",
            "  TEST HR@10 = 0.2978\n",
            "\n",
            "-- fold 3\n",
            "Split sizes: train=804987, val=97312, test=97312\n",
            "#users=6040, #items=3415\n",
            "  ep 20 val HR@10 = 0.2760\n",
            "  ep 40 val HR@10 = 0.2838\n",
            "  ep 60 val HR@10 = 0.3270\n",
            "  ep 80 val HR@10 = 0.3285\n",
            "  ep 100 val HR@10 = 0.3339\n",
            "  ep 120 val HR@10 = 0.3424\n",
            "  ep 140 val HR@10 = 0.3379\n",
            "  ep 160 val HR@10 = 0.3366\n",
            "  ep 180 val HR@10 = 0.3394\n",
            "  ep 200 val HR@10 = 0.3478\n",
            "  TEST HR@10 = 0.3061\n",
            "\n",
            "Graph CV HR@10 = 0.3015 ± 0.0035\n",
            "\n",
            "=== Graph CV (3 folds) ===\n",
            "\n",
            "-- fold 1\n",
            "Split sizes: train=804987, val=97312, test=97312\n",
            "#users=6040, #items=3415\n",
            "  ep 20 val HR@10 = 0.0253\n",
            "  ep 40 val HR@10 = 0.0253\n",
            "  ep 60 val HR@10 = 0.0253\n",
            "  ep 80 val HR@10 = 0.0253\n",
            "  ep 100 val HR@10 = 0.0253\n",
            "  ep 120 val HR@10 = 0.0253\n",
            "  early stopping\n",
            "  TEST HR@10 = 0.0323\n",
            "\n",
            "-- fold 2\n",
            "Split sizes: train=804987, val=97312, test=97312\n",
            "#users=6040, #items=3415\n",
            "  ep 20 val HR@10 = 0.0253\n",
            "  ep 40 val HR@10 = 0.0253\n",
            "  ep 60 val HR@10 = 0.0253\n",
            "  ep 80 val HR@10 = 0.0253\n",
            "  ep 100 val HR@10 = 0.0253\n",
            "  ep 120 val HR@10 = 0.0253\n",
            "  early stopping\n",
            "  TEST HR@10 = 0.0323\n",
            "\n",
            "-- fold 3\n",
            "Split sizes: train=804987, val=97312, test=97312\n",
            "#users=6040, #items=3415\n",
            "  ep 20 val HR@10 = 0.0253\n",
            "  ep 40 val HR@10 = 0.0253\n",
            "  ep 60 val HR@10 = 0.0253\n",
            "  ep 80 val HR@10 = 0.0253\n",
            "  ep 100 val HR@10 = 0.0253\n",
            "  ep 120 val HR@10 = 0.0253\n",
            "  early stopping\n",
            "  TEST HR@10 = 0.0323\n",
            "\n",
            "Graph CV HR@10 = 0.0323 ± 0.0000\n",
            "\n",
            "=== SASRec CV (3 folds) ===\n",
            "\n",
            "-- fold 1\n",
            "Split sizes: train=804987, val=97312, test=97312\n",
            "#users=6040, #items=3415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-88edecf89341>:367: RuntimeWarning: invalid value encountered in cast\n",
            "  seq = (grp.movie_idx.values + 1).clip(1, num_items).astype(int).tolist()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ep 20 val HR@10 = 0.2002\n",
            "  ep 40 val HR@10 = 0.2002\n",
            "  ep 60 val HR@10 = 0.2002\n",
            "  ep 80 val HR@10 = 0.2002\n",
            "  ep 100 val HR@10 = 0.2002\n",
            "  ep 120 val HR@10 = 0.2002\n",
            "  early stopping\n",
            "  TEST HR@10 = 0.1907\n",
            "\n",
            "-- fold 2\n",
            "Split sizes: train=804987, val=97312, test=97312\n",
            "#users=6040, #items=3415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-88edecf89341>:367: RuntimeWarning: invalid value encountered in cast\n",
            "  seq = (grp.movie_idx.values + 1).clip(1, num_items).astype(int).tolist()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ep 20 val HR@10 = 0.2002\n",
            "  ep 40 val HR@10 = 0.2002\n",
            "  ep 60 val HR@10 = 0.2002\n",
            "  ep 80 val HR@10 = 0.2002\n",
            "  ep 100 val HR@10 = 0.2002\n",
            "  ep 120 val HR@10 = 0.2002\n",
            "  early stopping\n",
            "  TEST HR@10 = 0.1907\n",
            "\n",
            "-- fold 3\n",
            "Split sizes: train=804987, val=97312, test=97312\n",
            "#users=6040, #items=3415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-88edecf89341>:367: RuntimeWarning: invalid value encountered in cast\n",
            "  seq = (grp.movie_idx.values + 1).clip(1, num_items).astype(int).tolist()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ep 20 val HR@10 = 0.2002\n",
            "  ep 40 val HR@10 = 0.2002\n",
            "  ep 60 val HR@10 = 0.2002\n",
            "  ep 80 val HR@10 = 0.2002\n",
            "  ep 100 val HR@10 = 0.2002\n",
            "  ep 120 val HR@10 = 0.2002\n",
            "  early stopping\n",
            "  TEST HR@10 = 0.1907\n",
            "\n",
            "SASRec CV HR@10 = 0.1907 ± 0.0000\n",
            "\n",
            "=== SUMMARY ===\n",
            "GATRec   HR@10 = 0.3015 ± 0.0035\n",
            "LightGCN HR@10 = 0.0323 ± 0.0000\n",
            "SASRec   HR@10 = 0.1907 ± 0.0000\n"
          ]
        }
      ]
    }
  ]
}